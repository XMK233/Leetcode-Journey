{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "moudule\\_description: __explain what does the name of this module mean (aka, explain the parameters displayed on their name)__ \n",
    "\n",
    "family\\_description: __copy a sentence from the original description, or do some necessary change__\n",
    "\n",
    "application\\_domain: __input domain and functionality__\n",
    "* input domain\n",
    "    * text\n",
    "    * audio\n",
    "    * video\n",
    "    * image\n",
    "    * structured\n",
    "* functionality: \n",
    "    * augmentation\n",
    "    * representation\n",
    "    * embedding\n",
    "    * generate\n",
    "    * discriminate\n",
    "    * classification\n",
    "    * feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "progress: \n",
    "\n",
    "2019年7月9日 imagenet-amoebanet is just finished. \n",
    "\n",
    "2019年7月10日: 完成了。从imagenet-inception-v1开始，使用的是自动化生成的信息；之前的都是手动复制粘贴生成的信息。有朝一日有空的话可以将所有的module都改为自动化生成信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xxx = '''Dataset: CelebA HQ\n",
    "Model: Non-saturating GAN\n",
    "Architecture: ResNet19\n",
    "Optimizer: Adam (lr=1.000e-04, beta1=0.500, beta2=0.900)\n",
    "Discriminator iterations per generator iteration: 5\n",
    "Discriminator normalizaton: Layer normalization\n",
    "Discriminator regularization: DRAGAN Gradient Penalty (lambda=1.000)'''\n",
    "print(xxx.replace(\"\\n\", \"; \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Family-product-link/version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delf\n",
      "elmo\n",
      "faster_rcnn-openimages_v4-inception_resnet_v2\n",
      "ganeval-cifar10-convnet\n",
      "inaturalist-inception_v3-feature_vector\n",
      "openimages_v4-ssd-mobilenet_v2\n",
      "progan-128\n",
      "223\n"
     ]
    }
   ],
   "source": [
    "import os, json, md2py\n",
    "import os, shutil, re, json, tqdm\n",
    "from lxml import etree\n",
    "from pprint import pprint\n",
    "\n",
    "JSON_FILE = r\"J:\\ModelStoreData\\AIHub\\2019-06-04\\TFModule_Info.json\"\n",
    "\n",
    "LATEST_MODULE_BASIC_INFO = {}\n",
    "\n",
    "def model_family_by_name():\n",
    "    modules = []\n",
    "    \n",
    "    with open(JSON_FILE, \"r\") as jf:\n",
    "        items = json.load(jf)\n",
    "        \n",
    "    for item in items:\n",
    "        name = item[\"name\"]\n",
    "        version = item[\"version\"]\n",
    "        info = item[\"info\"]\n",
    "        md = info[1][7]\n",
    "        link_url = \"https://aihub.cloud.google.com/p/products%2F{}\".format(info[1][1].split(\"/\")[1])\n",
    "        ############################\n",
    "        if name not in modules:\n",
    "            modules.append(name)\n",
    "        ############################\n",
    "        if name in LATEST_MODULE_BASIC_INFO and version <= LATEST_MODULE_BASIC_INFO[name][\"version\"]:\n",
    "            continue\n",
    "        LATEST_MODULE_BASIC_INFO[name] = {\n",
    "            \"version\": version, \n",
    "            \"md\": md, \n",
    "            \"link_url\": link_url\n",
    "        }\n",
    "        ###########################   \n",
    "    \n",
    "    modules.sort()\n",
    "#     pprint(modules)\n",
    "#     print(len(modules))\n",
    "    return modules\n",
    "    \n",
    "modules = model_family_by_name()\n",
    "\n",
    "def count_items_in_list_in_dict(dic):\n",
    "    count = 0\n",
    "    for i in dic:\n",
    "        count += (len(dic[i]) - 1)\n",
    "    return count\n",
    "\n",
    "families = {\n",
    "    \"Wiki-words\" : {\"family_description\": \"\"},\n",
    "    \"bert\" : {\"family_description\": \"\"},\n",
    "    \"biggan\" : {\"family_description\": \"\"},\n",
    "    \"biggan-deep\" : {\"family_description\": \"\"},\n",
    "    \"compare_gan-model\" : {\"family_description\": \"\"},\n",
    "    \"compare_gan-s3gan\" : {\"family_description\": \"\"},\n",
    "    \"i3d\" : {\"family_description\": \"\"},\n",
    "    \"image_augmentation\" : {\"family_description\": \"\"},\n",
    "    \"image_augmentation-nas\" : {\"family_description\": \"\"},\n",
    "    \"imagenet-amoebanet\" : {\"family_description\": \"\"},\n",
    "    \"imagenet-inception_resnet_v2\" : {\"family_description\": \"\"},\n",
    "    \"imagenet-inception_v1\" : {\"family_description\": \"Inception V1 (a.k.a. GoogLeNet) is a neural network architecture for image classification\"},\n",
    "    \"imagenet-inception_v2\" : {\"family_description\": \"Inception V2 is a neural network architecture for image classification; Inception V2 uses are more powerful architecture made possible by the use of batch normalization\"},\n",
    "    \"imagenet-inception_v3\" : {\"family_description\": \"Inception V3 is a neural network architecture for image classification\"},\n",
    "    \"imagenet-mobilenet_v1\" : {\"family_description\": \"MobileNet V1 is a family of neural network architectures for efficient on-device image classification; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed\"},\n",
    "    \"quantops\" : {\"family_description\": \"MobileNet V1 is a family of neural network architectures for efficient on-device image classification; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed; The implementation is instrumented for quantization\"},\n",
    "    \"imagenet-mobilenet_v2\" : {\"family_description\": \"MobileNet V2 is a family of neural network architectures for efficient on-device image classification and related tasks; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed\"},\n",
    "    \"imagenet-nasnet\" : {\"family_description\": \"NASNet-A is a family of convolutional neural networks for image classification. The architecture of its convolutional cells (or layers) has been found by Neural Architecture Search (NAS); NASNets come in various sizes\"},\n",
    "    \"imagenet-pnasnet\" : {\"family_description\": \"PNASNet-5 is a family of convolutional neural networks for image classification; The architecture of its convolutional cells (or layers) has been found by Progressive Neural Architecture Search; PNASNet reuses several techniques from is precursor NASNet, including regularization by path dropout\"},\n",
    "    \"imagenet-resnet_v1\" : {\"family_description\": \"ResNet (later renamed ResNet V1) is a family of network architectures for image classification with a variable number of layers\"},\n",
    "    \"imagenet-resnet_v2\" : {\"family_description\": \"ResNet V2 is a family of network architectures for image classification with a variable number of layers. It builds on the ResNet architecture; The key difference compared to ResNet V1 is the use of batch normalization before every weight layer\"},\n",
    "    \"nnlm\" : {\"family_description\": \"Text embedding based on feed-forward Neural-Net Language Models with pre-built OOV\"},\n",
    "    \"random-nnlm\" : {\"family_description\": 'Text embedding initialized with some random normal tensor; It contains no \"knowledge\", but can conveniently be used as a baseline when comparing to other modules; Vocabulary of the module is based on other nnlm modules in TFHub'},\n",
    "    \"tf2-preview-gnews-swivel\": {\"family_description\": \"SavedModel 2.0 format; help preview TensorFlow 2.0 functionality; Text embedding based on Swivel co-occurrence matrix factorization with pre-built OOV\"},\n",
    "    \"tf2-preview-inception_v3\": {\"family_description\": \"SavedModel 2.0 format; help preview TensorFlow 2.0 functionality; Inception V3 is a neural network architecture for image classification\"},\n",
    "    \"tf2-preview-mobilenet_v2\": {\"family_description\": \"SavedModel 2.0 format; help preview TensorFlow 2.0 functionality; MobileNet V2 is a family of neural network architectures for efficient on-device image classification and related tasks; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed\"},\n",
    "    \"tf2-preview-nnlm\": {\"family_description\": \"SavedModel 2.0 format; help preview TensorFlow 2.0 functionality; Text embedding based on feed-forward Neural-Net Language Models with pre-built OOV\"},\n",
    "    \"universal-sentence-encoder\" : {\"family_description\": \"The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks\"},\n",
    "    \"universal-sentence-encoder-xling\" : {\"family_description\": \"The Universal Sentence Encoder Cross-lingual (XLING) module is an extension of the Universal Sentence Encoder that includes training on multiple tasks across languages\"},\n",
    "}\n",
    "family_names = list(families.keys())\n",
    "family_names.sort()\n",
    "for module in modules:\n",
    "    ## check if the module name contains any family name. \n",
    "    belong_to_a_family = False\n",
    "    for family_name in family_names:\n",
    "        if family_name in module:\n",
    "            ## eliminate some duplication: random-nnlm/nnlm,,,\n",
    "            if family_name == \"nnlm\" and (\"random\" in module or \"tf2-preview\" in module):\n",
    "                continue\n",
    "            if family_name == \"biggan\" and (\"deep\" in module):\n",
    "                continue\n",
    "            if family_name == \"imagenet-mobilenet_v1\" and (\"quantops\" in module):\n",
    "                continue\n",
    "            if family_name == \"image_augmentation\" and (\"image_augmentation-nas\" in module):\n",
    "                continue\n",
    "            if family_name == \"universal-sentence-encoder\" and (\"xling\" in module):\n",
    "                continue \n",
    "            ## \n",
    "            families[family_name][module] = None\n",
    "            belong_to_a_family = True\n",
    "    if not belong_to_a_family:\n",
    "        print(module)\n",
    "\n",
    "print(count_items_in_list_in_dict(families))\n",
    "# pprint(families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This module is about 1GB\n",
      "This module is about 800MB\n",
      "lightweight version\n"
     ]
    }
   ],
   "source": [
    "for family_name in family_names:\n",
    "    modules = families.get(family_name)\n",
    "#     with open(\"{} {}.md\".format(len(modules), family_name), \"w\", encoding=\"utf-8\") as fn:\n",
    "    count = 0\n",
    "    for module in modules:\n",
    "        if module == \"family_description\":\n",
    "            continue\n",
    "        count += 1\n",
    "        module_info = LATEST_MODULE_BASIC_INFO[module]\n",
    "        link_url = module_info[\"link_url\"]\n",
    "        md = module_info[\"md\"]\n",
    "        version = module_info[\"version\"]\n",
    "    ###############\n",
    "    ###############################################\n",
    "        if family_name == \"imagenet-mobilenet_v1\":\n",
    "#             numbers = re.findall(\"\\d+_\\d+_\\d+\", module)[0]\n",
    "#             multiplier = float(numbers.split(\"_\")[-2]) / 100\n",
    "#             default_input_size = numbers.split(\"_\")[-1]\n",
    "            impl_info = re.findall(\"TF-Slim\\s*implementation\\s*of\\s*`[\\s\\S]*`\\s*with\\s*a\\s*depth\\s*multiplier\\s*of[\\s\\S]*?pixels\", md)\n",
    "            impl_info_string = impl_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "            if \"classification\" in module:\n",
    "                output_info = re.findall(\"num_classes` = \\d+\\s*classes\\s*of\\s*the\\s*classification\\s*from\\s*the\\s*original\\s*training\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\\s*by\\s*default\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; classification\", \n",
    "                    \"module_description\": \"{}; {}; {}\".format(impl_info_string, output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }\n",
    "\n",
    "            elif \"feature_vector\" in module:\n",
    "                output_info = re.findall(\"feature\\s*vector\\s*of\\s*size\\s*\\S+\\s*=\\s*\\d+\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\\s*by\\s*default\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; feature_vector\", \n",
    "                    \"module_description\": \"{}; {}; {}\".format(impl_info_string, output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }\n",
    "        elif family_name == \"imagenet-mobilenet_v2\" or family_name == \"quantops\" or family_name == \"tf2-preview-mobilenet_v2\":\n",
    "            ## the difference is that: \n",
    "            ##  the input size is fixed. While Mobilenet_v1 is set by default to ???x???. \n",
    "            impl_info = re.findall(\"TF-Slim\\s*implementation\\s*of\\s*`[\\s\\S]*`[\\s\\S]*with\\s*a\\s*depth\\s*multiplier\\s*of[\\s\\S]*?pixels\", md)\n",
    "            impl_info_string = impl_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \").replace(\"*\", \"\")\n",
    "            if \"classification\" in module:\n",
    "                output_info = re.findall(\"num_classes` = \\d+\\s*classes\\s*of\\s*the\\s*classification\\s*from\\s*the\\s*original\\s*training\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; classification\", \n",
    "                    \"module_description\": \"{}; {}; {}\".format(impl_info_string, output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }\n",
    "\n",
    "            elif \"feature_vector\" in module:\n",
    "                output_info = re.findall(\"feature\\s*vector\\s*[\\s\\S]*\\s*size\\s*\\S+\\s*=\\s*\\d+\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; feature_vector\", \n",
    "                    \"module_description\": \"{}; {}; {}\".format(impl_info_string, output_info_string, input_info_string),\n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }\n",
    "        elif family_name == \"imagenet-nasnet\" or family_name == \"imagenet-pnasnet\":\n",
    "#             print(md)\n",
    "            nn_detail = re.findall(\"This\\s*TF-Hub\\s*module\\s*uses\\s*the\\s*TF-Slim[\\s\\S]*of\\s*\\d+x\\d+\\s*pixels\", md)\n",
    "            nn_detail_string = nn_detail[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "            if \"classification\" in module:\n",
    "                output_info = re.findall(\"num_classes` = \\d+\\s*classes\\s*of\\s*the\\s*classification\\s*from\\s*the\\s*original\\s*training\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; classification\", \n",
    "                    \"module_description\": \"{}; {}; {}\".format(nn_detail_string, output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }\n",
    "\n",
    "            elif \"feature_vector\" in module:\n",
    "                output_info = re.findall(\"feature\\s*vector\\s*of\\s*size\\s*\\S+\\s*=\\s*\\d+\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; feature_vector\", \n",
    "                    \"module_description\": \"{}; {}; {}\".format(nn_detail_string, output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }   \n",
    "        elif family_name == \"imagenet-resnet_v1\" or family_name == \"imagenet-resnet_v2\":\n",
    "            nn_detail = re.findall(\"This\\s*TF-Hub\\s*module\\s*uses\\s*the\\s*TF-Slim[\\s\\S]*layers\", md)\n",
    "            nn_detail_string = nn_detail[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "            if \"classification\" in module:\n",
    "                output_info = re.findall(\"num_classes` = \\d+\\s*classes\\s*of\\s*the\\s*classification\\s*from\\s*the\\s*original\\s*training\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\\s*by\\s*default\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; classification\", \n",
    "                    \"module_description\": \"{}; {}; {}\".format(nn_detail_string, output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }\n",
    "\n",
    "            elif \"feature_vector\" in module:\n",
    "                output_info = re.findall(\"feature\\s*vector\\s*of\\s*size\\s*\\S+\\s*=\\s*\\d+\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\\s*by\\s*default\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; feature_vector\", \n",
    "                    \"module_description\": \"{}; {}; {}\".format(nn_detail_string, output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }   \n",
    "        elif family_name == \"nnlm\" or family_name == \"tf2-preview-nnlm\":\n",
    "            splt = module.split(\"-\")\n",
    "            language = splt[splt.index(\"nnlm\") + 1]\n",
    "            dimension = re.findall(\"Maps\\s*from\\s*text\\s*to\\s*([\\s\\S]*?)embedding\\s*vectors.\", md)\n",
    "            dimension_string = \"Map from text to {} vectors\".format(dimension[0].strip())\n",
    "            hidden_layers = re.findall(\"Details\\s*([\\s\\S]*?).\\s*####\\s*Input\", md) #re.findall(\"Based\\s*on\\s*NNLM\\s*with\\s*([\\s\\S]*?)layers\", md)\n",
    "            hidden_layers_string = hidden_layers[0] #\"Based on NNLM with {} layers\".format(hidden_layers[0].strip())\n",
    "            preprocessing = re.findall(\"The\\s*module\\s*preprocesses\\s*its\\s*input\\s*by([\\s\\S]*?).\\n\", md)\n",
    "            preprocessing_string = \"Input preprocessed by {}\".format(preprocessing[0].strip().replace(\"*\", \"\")) \n",
    "            families[family_name][module] = {\n",
    "                    \"application_domain\": \"text; embedding\", \n",
    "                    \"module_description\": \"Language: {}; {}; {}; {}\".format(language, dimension_string, hidden_layers_string, preprocessing_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }  \n",
    "        elif family_name == \"random-nnlm\":\n",
    "            initailization = re.findall(\"Text\\s*embedding\\s*initialized\\s*with\\s*`([\\s\\S]*?)`\", md)\n",
    "            initailization_string = \"Text embedding initialized with {}\".format(initailization[0].strip())\n",
    "            voca = re.findall(\"Vocabulary\\s*of\\s*the\\s*module\\s*is\\s*based\\s*on\\s*\\[([\\s\\S]*?)\\]\", md)\n",
    "            voca_string = \"Vocabulary of the module is based on {}\".format(voca[0].strip())\n",
    "            families[family_name][module] = {\n",
    "                    \"application_domain\": \"text; embedding\", \n",
    "                    \"module_description\": \"{}; {}\".format(initailization_string, voca_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }  \n",
    "        elif family_name == \"tf2-preview-gnews-swivel\":\n",
    "            dimension = re.findall(\"Maps\\s*from\\s*text\\s*to\\s*([\\s\\S]*?)embedding\\s*vectors.\", md)\n",
    "            dimension_string = \"Map from text to {} vectors\".format(dimension[0].strip())\n",
    "            details = re.findall(\"Details\\s*([\\s\\S]*?).\\s*####\\s*Input\", md) #\"Created using Swivel matrix factorization method\"\n",
    "            details_string = details[0]\n",
    "            preprocessing = re.findall(\"The\\s*module\\s*preprocesses\\s*its\\s*input\\s*by([\\s\\S]*?).\\n\", md)\n",
    "            preprocessing_string = \"Input preprocessed by {}\".format(preprocessing[0].strip().replace(\"*\", \"\")) \n",
    "            families[family_name][module] = {\n",
    "                    \"application_domain\": \"text; embedding\", \n",
    "                    \"module_description\": \"{}; {}; {}\".format(dimension_string, details_string, preprocessing_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                } \n",
    "        elif family_name == \"imagenet-inception_v1\" or family_name == \"imagenet-inception_v2\" or family_name == \"imagenet-inception_v3\":\n",
    "            if \"classification\" in module:\n",
    "                output_info = re.findall(\"num_classes` = \\d+\\s*classes\\s*of\\s*the\\s*classification\\s*from\\s*the\\s*original\\s*training\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; classification\", \n",
    "                    \"module_description\": \"{}; {}\".format(output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }\n",
    "\n",
    "            elif \"feature_vector\" in module:\n",
    "                output_info = re.findall(\"feature\\s*vector\\s*of\\s*size\\s*\\S+\\s*=\\s*\\d+\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; feature_vector\", \n",
    "                    \"module_description\": \"{}; {}\".format(output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }\n",
    "        elif family_name == \"tf2-preview-inception_v3\":\n",
    "            if \"classification\" in module:\n",
    "                output_info = re.findall(\"num_classes` = \\d+\\s*classes\\s*of\\s*the\\s*classification\\s*from\\s*the\\s*original\\s*training\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; classification\", \n",
    "                    \"module_description\": \"{}; {}\".format(output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }\n",
    "\n",
    "            elif \"feature_vector\" in module:\n",
    "                output_info = re.findall(\"feature\\s*vector\\s*has\\s*size\\s*\\S+\\s*=\\s*\\d+\", md)\n",
    "                output_info_string = output_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                input_info = re.findall(\"size\\s*of\\s*the\\s*input\\s*images\\s*is[\\s\\S]*pixels\", md)\n",
    "                input_info_string = input_info[0].replace(\"`\", \"\").replace(\"\\n\", \" \")\n",
    "                families[family_name][module] = {\n",
    "                    \"application_domain\": \"image; feature_vector\", \n",
    "                    \"module_description\": \"{}; {}\".format(output_info_string, input_info_string), \n",
    "                    \"link\": link_url, \n",
    "                    \"version\": version\n",
    "                }   \n",
    "        elif family_name == \"universal-sentence-encoder\":            \n",
    "            output_info = re.findall(\"output\\s*is\\s*a[\\s\\S]*?dimensional\\s*vector\", md)\n",
    "            output_info_string = output_info[0]\n",
    "            other_description = \"This module is about 800MB\" if \"large\" in module else \"lightweight version\" if \"lite\" in module else \"This module is about 1GB\" \n",
    "            print(other_description)\n",
    "            families[family_name][module] = {\n",
    "                \"application_domain\": \"text; embedding\", \n",
    "                \"module_description\": \"{}; {}\".format(other_description, output_info_string), \n",
    "                \"link\": link_url, \n",
    "                \"version\": version\n",
    "            }\n",
    "        elif family_name == \"universal-sentence-encoder-xling\": \n",
    "            languages = re.findall(\"\\*\\*([\\s\\S]*)\\*\\*\", md)\n",
    "            languages_string = languages[0].replace(\"\\n\", \" \")\n",
    "            output_info = re.findall(\"output\\s*is\\s*a[\\s\\S]*dimensional\\s*vector\", md)\n",
    "            output_info_string = output_info[0]\n",
    "            families[family_name][module] = {\n",
    "                \"application_domain\": \"text; embedding\", \n",
    "                \"module_description\": \"{}; {}\".format(languages_string, output_info_string), \n",
    "                \"link\": link_url, \n",
    "                \"version\": version\n",
    "            }\n",
    "        else:\n",
    "            families[family_name][module] = {\"application_domain\": \"\", \"module_description\": \"\", \"link\": link_url, \"version\": version}\n",
    "    ###############################################\n",
    "# pprint(families)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则表达式生成器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xmk233\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: bad escape \\s\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\\s*is\\s*a[\\s\\S]*dimensional\\s*vector\n"
     ]
    }
   ],
   "source": [
    "'''* Maps from text to [\\s\\S]*embedding vectors\n",
    "* Based on NNLM with [\\s\\S]*layers\n",
    "* The module preprocesses its input by[\\s\\S]*.'''\n",
    "\n",
    "pp = re.compile(\"\\s\")\n",
    "\n",
    "print(pp.sub(\"\\\\s*\", '''output is a[\\s\\S]*dimensional vector'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## product-changelog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "182\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "def get_element_by_xpath(page_source, xpath, index = 0):\n",
    "    res = None\n",
    "    try:\n",
    "        _ = page_source.xpath(xpath)[index]\n",
    "        ## eliminate unnecessary chars.\n",
    "        _ = _.replace(\"\\n\", \"\").strip()\n",
    "        ## change the strings to lower case. Hopefully it will make the comparison standardized.\n",
    "        ## be careful about this change. If it is not suitable to the situation, stop it without hesitation.\n",
    "        _ = _.lower()\n",
    "        res = ' '.join(_.split())\n",
    "    except:\n",
    "        res = None\n",
    "    finally:\n",
    "        return res\n",
    "\n",
    "def parse_html_specially_for_imagenet(html, *args):\n",
    "    res = None\n",
    "    try:\n",
    "        page_source = etree.HTML(html)\n",
    "        author = get_element_by_xpath(page_source, \"//ul/li/text()\", args[0])\n",
    "        author = author.replace(\":\", \"\")\n",
    "        paper = get_element_by_xpath(page_source, \"//ul/li/a/text()\", args[1])\n",
    "        paper = paper.replace(\"\\\"\", \"\")\n",
    "        link_to_paper = get_element_by_xpath(page_source, \"//ul/li/a/@href\", args[2])\n",
    "        year = get_element_by_xpath(page_source, \"//ul/li/text()\", args[3])\n",
    "        year = re.findall(r\"(\\d{4})\", year)[0]\n",
    "        res = {\"author\": author, \"paper\": paper, \"link_to_paper\":link_to_paper, \"year\": year}\n",
    "    except:\n",
    "        print(f\"failed\")\n",
    "    finally:\n",
    "        return res\n",
    "    \n",
    "model_version_logInfo = {}\n",
    "model_logInfo = {}\n",
    "\n",
    "\n",
    "def Gather_md_code():\n",
    "    with open(JSON_FILE, \"r\") as jf:\n",
    "        items = json.load(jf)\n",
    "    counting_hangelog = 0\n",
    "    counting_ersion = 0\n",
    "    counting_other = 0\n",
    "    for item in items:\n",
    "        name = item[\"name\"]\n",
    "        version = item[\"version\"]\n",
    "        info = item[\"info\"]\n",
    "        link_url = \"https://aihub.cloud.google.com/p/products%2F{}/v/{}\".format(info[1][1].split(\"/\")[1], version)\n",
    "        md = info[1][7]\n",
    "        # toc = md2py.md2py(md)\n",
    "        # page_source = etree.HTML(str(toc.source))\n",
    "\n",
    "        ## lets' see whether they have c\"hangelog\" keywords.\n",
    "        if \"hangelog\" in md:\n",
    "            counting_hangelog += 1\n",
    "            toc1 = md2py.md2py(md.split(\"hangelog\")[1])\n",
    "            source_to_write = str(toc1.source)\n",
    "            remove_references = re.split(\"<h\\d>\\Seferences</h\\d>\", source_to_write)[0]\n",
    "\n",
    "            model_logInfo[name] = remove_references\n",
    "\n",
    "            if name not in model_version_logInfo:\n",
    "                model_version_logInfo[name] = {}  # [[version, str(toc1.source)]]\n",
    "                model_version_logInfo[name][version] = remove_references  # str(toc1.source)\n",
    "            else:\n",
    "                #                 model_version_logInfo[name].append([version, str(toc1.source)])\n",
    "                model_version_logInfo[name][version] = remove_references  # str(toc1.source)\n",
    "        #             print(str(toc1.source))\n",
    "        #             print(name, version, link_url)\n",
    "        else:\n",
    "            ## if there is no changelog keyword, let's see \"version\" keyword.\n",
    "            ## .. After manually checking out, no version/update/log.. info can be found.\n",
    "            if \"ersion\" in md:\n",
    "                counting_ersion += 1\n",
    "                # print(md, \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "                # print(name, version, link_url)\n",
    "            else:\n",
    "                ## if neither \"ersion\" nor \"hangelog\" is available:\n",
    "                ## .. After manually checking out, no version/update/log.. info can be found.\n",
    "                counting_other += 1\n",
    "                # print(name, version, link_url)\n",
    "                # print(md, \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "    print(counting_hangelog)\n",
    "    print(counting_ersion)\n",
    "    print(counting_other)\n",
    "\n",
    "\n",
    "Gather_md_code()\n",
    "# pprint(model_logInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for family_name in family_names:\n",
    "    modules = families.get(family_name)\n",
    "#     with open(\"{} {}.md\".format(len(modules), family_name), \"w\", encoding=\"utf-8\") as fn:\n",
    "    count = 0\n",
    "    for module in modules:\n",
    "        if module == \"family_description\":\n",
    "            continue\n",
    "        count += 1\n",
    "    ###############\n",
    "        families[family_name][module][\"changelog\"] = model_logInfo.get(module)\n",
    "# pprint(families)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## product-references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 modules are not in there: \n",
    "\n",
    "#### 3 models don't have any reference info: \n",
    "\n",
    "(They don't clarify their references. Though their webpages look like the previous three, but I don't regard so for certain. )\n",
    "\n",
    "image_augmentation-crop_rotate_color 1 https://aihub.cloud.google.com/p/products%2Fcd9eda73-8cc4-4dbc-aa62-c97e98d657af\n",
    "\n",
    "image_augmentation-crop_color 1 https://aihub.cloud.google.com/p/products%2F83bb37d7-5060-4ba7-85e4-6c00a582e789\n",
    "\n",
    "image_augmentation-flipx_crop_rotate_color 1 https://aihub.cloud.google.com/p/products%2F2012b39c-9bd7-4670-bbc0-190c89405778\n",
    "\n",
    "#### 2 models clarify their referred stuff: \n",
    "faster_rcnn-openimages_v4-inception_resnet_v2 1 https://aihub.cloud.google.com/p/products%2F41b42dfa-e600-4a73-a425-7c5c4d511c3c\n",
    "* Only has some referred models and datasets: *FasterRCNN+InceptionResNetV2 network trained on Open Images V4*\n",
    "\n",
    "openimages_v4-ssd-mobilenet_v2 1 https://aihub.cloud.google.com/p/products%2F8c6878ba-d32d-411d-bac2-2f884b748c4f\n",
    "* Only has some referred models and datasets: *SSD+MobileNetV2 network trained on Open Images V4*\n",
    "\n",
    "#### 3 models don't have any reference info: \n",
    "\n",
    "ganeval-cifar10-convnet 1 https://aihub.cloud.google.com/p/products%2F110689d8-c594-49d6-aef4-9fc91c9c2ab4\n",
    "\n",
    "random-nnlm-en-dim50 1 https://aihub.cloud.google.com/p/products%2Fb0766fd5-82b5-48e3-9d6f-26c5d2601d5f\n",
    "\n",
    "random-nnlm-en-dim128 1 https://aihub.cloud.google.com/p/products%2Fa52e9466-f691-4f91-a62e-f6e39eb967f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ganeval-cifar10-convnet 1 https://aihub.cloud.google.com/p/products%2F110689d8-c594-49d6-aef4-9fc91c9c2ab4\n",
      "faster_rcnn-openimages_v4-inception_resnet_v2 1 https://aihub.cloud.google.com/p/products%2F41b42dfa-e600-4a73-a425-7c5c4d511c3c\n",
      "random-nnlm-en-dim50 1 https://aihub.cloud.google.com/p/products%2Fb0766fd5-82b5-48e3-9d6f-26c5d2601d5f\n",
      "image_augmentation-crop_rotate_color 1 https://aihub.cloud.google.com/p/products%2Fcd9eda73-8cc4-4dbc-aa62-c97e98d657af\n",
      "image_augmentation-nas_svhn 1 https://aihub.cloud.google.com/p/products%2F5eef4456-8aa9-47d7-84db-00a7faa6662a\n",
      "image_augmentation-crop_color 1 https://aihub.cloud.google.com/p/products%2F83bb37d7-5060-4ba7-85e4-6c00a582e789\n",
      "random-nnlm-en-dim128 1 https://aihub.cloud.google.com/p/products%2Fa52e9466-f691-4f91-a62e-f6e39eb967f8\n",
      "image_augmentation-nas_cifar 1 https://aihub.cloud.google.com/p/products%2F822fa7f1-2207-4645-bcab-b7b916dae368\n",
      "image_augmentation-nas_imagenet 1 https://aihub.cloud.google.com/p/products%2Fdaf153b0-cc1b-4f27-a037-5bd3d9fff224\n",
      "image_augmentation-flipx_crop_rotate_color 1 https://aihub.cloud.google.com/p/products%2F2012b39c-9bd7-4670-bbc0-190c89405778\n",
      "openimages_v4-ssd-mobilenet_v2 1 https://aihub.cloud.google.com/p/products%2F8c6878ba-d32d-411d-bac2-2f884b748c4f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def beautify_string(paper):\n",
    "    paper = paper.replace(\"\\\"\", \"\")\n",
    "    paper = paper.replace(\"\\n\", \" \").strip()\n",
    "    paper = ' '.join(paper.split())\n",
    "    return paper\n",
    "\n",
    "# def remove_github_items(l):\n",
    "#     for ll in l: \n",
    "#         if \"GitHub issue\" in ll:\n",
    "#             l.remove(ll)\n",
    "#         if \"https://github.com\" in ll:\n",
    "#             l.remove(ll)\n",
    "#     return l\n",
    "\n",
    "def remove_github_items(keywords, l):\n",
    "    removed_index = []\n",
    "    for i in range(len(l)): \n",
    "        if keywords in l[i]:\n",
    "            removed_index.append(i)\n",
    "    for i in sorted(removed_index, reverse=True):\n",
    "        del l[i]\n",
    "    return l\n",
    "#         print(re.findall(\"[G, g]it[H, h]ub\"))\n",
    "\n",
    "paper_link = {}\n",
    "module_link = {}\n",
    "paper_module = {}\n",
    "module_paper = {}\n",
    "module_version = {}\n",
    "\n",
    "def get_reference_info(module, md):\n",
    "    toc = md2py.md2py(md)\n",
    "    page_source = etree.HTML(str(toc.source))\n",
    "    papers = page_source.xpath(\"//li/a/text()\")\n",
    "    links = page_source.xpath(\"//li/a/@href\")\n",
    "    \n",
    "    ## the amoebanet modules may have strange paper name which is \"arXiv:1802.01548\". But the paper name can be got manually. \n",
    "    ## lets' omit this detail first. \n",
    "    ## if necessary, make some change. \n",
    "#     if \"amoebanet\" in module:\n",
    "#         pass        \n",
    "    \n",
    "    papers = list(map(beautify_string, papers))\n",
    "    papers = remove_github_items(\"GitHub issue\", papers)\n",
    "    links = remove_github_items(\"https://github.com\", links)\n",
    "\n",
    "    for paper, link in zip(papers, links):\n",
    "        paper_link[paper] = link\n",
    "\n",
    "        if paper not in paper_module:\n",
    "            paper_module[paper] = [module]\n",
    "        else:\n",
    "            if module not in paper_module[paper]:\n",
    "                paper_module[paper].append(module)\n",
    "#             if module not in module_paper:\n",
    "#                 module_paper[module] = [paper]\n",
    "#             else:\n",
    "#                 if paper not in module_paper[module]:\n",
    "#                     module_paper[module].append(paper)    \n",
    "    module_paper[module] = papers\n",
    "    \n",
    "def add_paper_into_dicts(paper, module):\n",
    "    if paper not in paper_module:\n",
    "        paper_module[paper] = [module]\n",
    "    else:\n",
    "        if module not in paper_module[paper]:\n",
    "            paper_module[paper].append(module)\n",
    "\n",
    "def getting_information():\n",
    "    with open(JSON_FILE, \"r\") as jf:\n",
    "        items = json.load(jf)\n",
    "    for item in items:\n",
    "        module = item[\"name\"]\n",
    "        version = item[\"version\"]\n",
    "        info = item[\"info\"]\n",
    "        link_url = \"https://aihub.cloud.google.com/p/products%2F{}\".format(info[1][1].split(\"/\")[1])\n",
    "        md = info[1][7]\n",
    "\n",
    "        if module in module_version:\n",
    "            module_version[module].append(version)\n",
    "        else:\n",
    "            module_version[module] = [version]\n",
    "\n",
    "        module_link[module] = link_url\n",
    "\n",
    "        if \"imagenet\" in module.split(\"-\")[0]:    \n",
    "            get_reference_info(module, md)\n",
    "        elif \"eference\" in md:\n",
    "            toc = md2py.md2py(md)\n",
    "            page_source = etree.HTML(str(toc.source))\n",
    "            ## paper name could be in the following two xpathes.\n",
    "    #         combi1 = page_source.xpath(\"//p[last()]/text()\")\n",
    "    #         print(combi1)\n",
    "            combi2 = page_source.xpath(\"//a[last()]/text()\")\n",
    "            ## the best part is in \"paper\"\n",
    "            if \"universal-sentence-encoder-xling\" in module:\n",
    "                paper1 = \"Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model.\" ## combi1 will be: ['[1] M. Chidambaram, Y. Yang, D. Cer, S. Yuan, Y.-H. Sung, B. Strope, and R.\\nKurzweil. Learning Cross-Lingual Sentence Representations via a Multi-task\\nDual-Encoder Model. ArXiv e-prints, October 2018.']\n",
    "                add_paper_into_dicts(paper1, module)\n",
    "                paper2 = \"Universal Sentence Encoder\"\n",
    "                add_paper_into_dicts(paper2, module)\n",
    "                module_paper[module] = [paper1, paper2]\n",
    "            elif \"delf\" in module:\n",
    "                papers = list(map(beautify_string, combi2))\n",
    "                for paper in papers:\n",
    "                    add_paper_into_dicts(paper, module)\n",
    "                module_paper[module] = papers\n",
    "            else:\n",
    "                paper = combi2[-1].replace(\"\\n\", \"\")\n",
    "                add_paper_into_dicts(paper, module)\n",
    "                module_paper[module] = [paper]\n",
    "        else:\n",
    "            if \"tf2-preview\" in module:\n",
    "                get_reference_info(module, md)\n",
    "            elif \"inaturalist-inception_v3\" in module:\n",
    "                get_reference_info(module, md)\n",
    "            else:\n",
    "                print(module, version, link_url)\n",
    "                pass\n",
    "\n",
    "getting_information()\n",
    "\n",
    "def add_paper_and_module(papers, modules):\n",
    "    for paper in papers:\n",
    "        paper_module[paper] = modules\n",
    "    for module in modules:\n",
    "        module_paper[module] = papers\n",
    "add_paper_and_module(\n",
    "    [\"AutoAugment: Learning Augmentation Policies from Data\"], \n",
    "    [\n",
    "        \"image_augmentation-nas_svhn\", \n",
    "        \"image_augmentation-nas_imagenet\", \n",
    "        \"image_augmentation-nas_cifar\"\n",
    "    ]\n",
    ")\n",
    "paper_link[\"AutoAugment: Learning Augmentation Policies from Data\"] = \"https://arxiv.org/abs/1805.09501\"\n",
    "module_link[\"image_augmentation-nas_svhn\"] = \"https://aihub.cloud.google.com/p/products%2F5eef4456-8aa9-47d7-84db-00a7faa6662a\"\n",
    "module_link[\"image_augmentation-nas_imagenet\"] = \"https://aihub.cloud.google.com/p/products%2Fdaf153b0-cc1b-4f27-a037-5bd3d9fff224\"\n",
    "module_link[\"image_augmentation-nas_cifar\"] = \"https://aihub.cloud.google.com/p/products%2F822fa7f1-2207-4645-bcab-b7b916dae368\"\n",
    "\n",
    "len(module_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(module_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for family_name in family_names:\n",
    "    modules = families.get(family_name)\n",
    "#     with open(\"{} {}.md\".format(len(modules), family_name), \"w\", encoding=\"utf-8\") as fn:\n",
    "    count = 0\n",
    "    for module in modules:\n",
    "        if module == \"family_description\":\n",
    "            continue\n",
    "        count += 1\n",
    "    ###############\n",
    "        families[family_name][module][\"references\"] = module_paper.get(module)\n",
    "# pprint(families)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print the result into file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"family_module_info_table.json\", \"w\", encoding = \"utf-8\") as fmit:\n",
    "    json.dump(families, fmit, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
