{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019年7月9日\n",
    "我们要做什么呢？\n",
    "* 首先，为每一个module找到它们的referenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ganeval-cifar10-convnet 1 https://aihub.cloud.google.com/p/products%2F110689d8-c594-49d6-aef4-9fc91c9c2ab4\n",
      "faster_rcnn-openimages_v4-inception_resnet_v2 1 https://aihub.cloud.google.com/p/products%2F41b42dfa-e600-4a73-a425-7c5c4d511c3c\n",
      "random-nnlm-en-dim50 1 https://aihub.cloud.google.com/p/products%2Fb0766fd5-82b5-48e3-9d6f-26c5d2601d5f\n",
      "image_augmentation-crop_rotate_color 1 https://aihub.cloud.google.com/p/products%2Fcd9eda73-8cc4-4dbc-aa62-c97e98d657af\n",
      "image_augmentation-nas_svhn 1 https://aihub.cloud.google.com/p/products%2F5eef4456-8aa9-47d7-84db-00a7faa6662a\n",
      "image_augmentation-crop_color 1 https://aihub.cloud.google.com/p/products%2F83bb37d7-5060-4ba7-85e4-6c00a582e789\n",
      "random-nnlm-en-dim128 1 https://aihub.cloud.google.com/p/products%2Fa52e9466-f691-4f91-a62e-f6e39eb967f8\n",
      "image_augmentation-nas_cifar 1 https://aihub.cloud.google.com/p/products%2F822fa7f1-2207-4645-bcab-b7b916dae368\n",
      "image_augmentation-nas_imagenet 1 https://aihub.cloud.google.com/p/products%2Fdaf153b0-cc1b-4f27-a037-5bd3d9fff224\n",
      "image_augmentation-flipx_crop_rotate_color 1 https://aihub.cloud.google.com/p/products%2F2012b39c-9bd7-4670-bbc0-190c89405778\n",
      "openimages_v4-ssd-mobilenet_v2 1 https://aihub.cloud.google.com/p/products%2F8c6878ba-d32d-411d-bac2-2f884b748c4f\n"
     ]
    }
   ],
   "source": [
    "import json, md2py, re\n",
    "from lxml import etree\n",
    "\n",
    "JSON_FILE = r\"J:\\ModelStoreData\\AIHub\\2019-06-04\\TFModule_Info.json\"\n",
    "\n",
    "def beautify_string(paper):\n",
    "    paper = paper.replace(\"\\\"\", \"\")\n",
    "    paper = paper.replace(\"\\n\", \" \").strip()\n",
    "    paper = ' '.join(paper.split())\n",
    "    return paper\n",
    "\n",
    "# def remove_github_items(l):\n",
    "#     for ll in l: \n",
    "#         if \"GitHub issue\" in ll:\n",
    "#             l.remove(ll)\n",
    "#         if \"https://github.com\" in ll:\n",
    "#             l.remove(ll)\n",
    "#     return l\n",
    "\n",
    "def remove_github_items(keywords, l):\n",
    "    removed_index = []\n",
    "    for i in range(len(l)): \n",
    "        if keywords in l[i]:\n",
    "            removed_index.append(i)\n",
    "    for i in sorted(removed_index, reverse=True):\n",
    "        del l[i]\n",
    "    return l\n",
    "#         print(re.findall(\"[G, g]it[H, h]ub\"))\n",
    "\n",
    "paper_link = {}\n",
    "module_link = {}\n",
    "paper_module = {}\n",
    "module_paper = {}\n",
    "module_version = {}\n",
    "\n",
    "def get_reference_info(module, md):\n",
    "    toc = md2py.md2py(md)\n",
    "    page_source = etree.HTML(str(toc.source))\n",
    "    papers = page_source.xpath(\"//li/a/text()\")\n",
    "    links = page_source.xpath(\"//li/a/@href\")\n",
    "    \n",
    "    ## the amoebanet modules may have strange paper name which is \"arXiv:1802.01548\". But the paper name can be got manually. \n",
    "    ## lets' omit this detail first. \n",
    "    ## if necessary, make some change. \n",
    "#     if \"amoebanet\" in module:\n",
    "#         pass        \n",
    "    \n",
    "    papers = list(map(beautify_string, papers))\n",
    "    papers = remove_github_items(\"GitHub issue\", papers)\n",
    "    links = remove_github_items(\"https://github.com\", links)\n",
    "\n",
    "    for paper, link in zip(papers, links):\n",
    "        paper_link[paper] = link\n",
    "\n",
    "        if paper not in paper_module:\n",
    "            paper_module[paper] = [module]\n",
    "        else:\n",
    "            if module not in paper_module[paper]:\n",
    "                paper_module[paper].append(module)\n",
    "#             if module not in module_paper:\n",
    "#                 module_paper[module] = [paper]\n",
    "#             else:\n",
    "#                 if paper not in module_paper[module]:\n",
    "#                     module_paper[module].append(paper)    \n",
    "    module_paper[module] = papers\n",
    "    \n",
    "def add_paper_into_dicts(paper, module):\n",
    "    if paper not in paper_module:\n",
    "        paper_module[paper] = [module]\n",
    "    else:\n",
    "        if module not in paper_module[paper]:\n",
    "            paper_module[paper].append(module)\n",
    "\n",
    "def getting_information():\n",
    "    with open(JSON_FILE, \"r\") as jf:\n",
    "        items = json.load(jf)\n",
    "    for item in items:\n",
    "        module = item[\"name\"]\n",
    "        version = item[\"version\"]\n",
    "        info = item[\"info\"]\n",
    "        link_url = \"https://aihub.cloud.google.com/p/products%2F{}\".format(info[1][1].split(\"/\")[1])\n",
    "        md = info[1][7]\n",
    "\n",
    "        if module in module_version:\n",
    "            module_version[module].append(version)\n",
    "        else:\n",
    "            module_version[module] = [version]\n",
    "\n",
    "        module_link[module] = link_url\n",
    "\n",
    "        if \"imagenet\" in module.split(\"-\")[0]:    \n",
    "            get_reference_info(module, md)\n",
    "        elif \"eference\" in md:\n",
    "            toc = md2py.md2py(md)\n",
    "            page_source = etree.HTML(str(toc.source))\n",
    "            ## paper name could be in the following two xpathes.\n",
    "    #         combi1 = page_source.xpath(\"//p[last()]/text()\")\n",
    "    #         print(combi1)\n",
    "            combi2 = page_source.xpath(\"//a[last()]/text()\")\n",
    "            ## the best part is in \"paper\"\n",
    "            if \"universal-sentence-encoder-xling\" in module:\n",
    "                paper1 = \"Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model.\" ## combi1 will be: ['[1] M. Chidambaram, Y. Yang, D. Cer, S. Yuan, Y.-H. Sung, B. Strope, and R.\\nKurzweil. Learning Cross-Lingual Sentence Representations via a Multi-task\\nDual-Encoder Model. ArXiv e-prints, October 2018.']\n",
    "                add_paper_into_dicts(paper1, module)\n",
    "                paper2 = \"Universal Sentence Encoder\"\n",
    "                add_paper_into_dicts(paper2, module)\n",
    "                module_paper[module] = [paper1, paper2]\n",
    "            elif \"delf\" in module:\n",
    "                papers = list(map(beautify_string, combi2))\n",
    "                for paper in papers:\n",
    "                    add_paper_into_dicts(paper, module)\n",
    "                module_paper[module] = papers\n",
    "            else:\n",
    "                paper = combi2[-1].replace(\"\\n\", \"\")\n",
    "                add_paper_into_dicts(paper, module)\n",
    "                module_paper[module] = [paper]\n",
    "        else:\n",
    "            if \"tf2-preview\" in module:\n",
    "                get_reference_info(module, md)\n",
    "            elif \"inaturalist-inception_v3\" in module:\n",
    "                get_reference_info(module, md)\n",
    "            else:\n",
    "                print(module, version, link_url)\n",
    "                pass\n",
    "\n",
    "getting_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paper_and_module(papers, modules):\n",
    "    for paper in papers:\n",
    "        paper_module[paper] = modules\n",
    "    for module in modules:\n",
    "        module_paper[module] = papers\n",
    "add_paper_and_module(\n",
    "    [\"AutoAugment: Learning Augmentation Policies from Data\"], \n",
    "    [\n",
    "        \"image_augmentation-nas_svhn\", \n",
    "        \"image_augmentation-nas_imagenet\", \n",
    "        \"image_augmentation-nas_cifar\"\n",
    "    ]\n",
    ")\n",
    "paper_link[\"AutoAugment: Learning Augmentation Policies from Data\"] = \"https://arxiv.org/abs/1805.09501\"\n",
    "module_link[\"image_augmentation-nas_svhn\"] = \"https://aihub.cloud.google.com/p/products%2F5eef4456-8aa9-47d7-84db-00a7faa6662a\"\n",
    "module_link[\"image_augmentation-nas_imagenet\"] = \"https://aihub.cloud.google.com/p/products%2Fdaf153b0-cc1b-4f27-a037-5bd3d9fff224\"\n",
    "module_link[\"image_augmentation-nas_cifar\"] = \"https://aihub.cloud.google.com/p/products%2F822fa7f1-2207-4645-bcab-b7b916dae368\"\n",
    "\n",
    "\n",
    "\n",
    "# paper_module[\"AutoAugment: Learning Augmentation Policies from Data\"] = [\n",
    "#     \"image_augmentation-nas_svhn\", \n",
    "#     \"image_augmentation-nas_imagenet\", \n",
    "#     \"image_augmentation-nas_cifar\"\n",
    "# ]\n",
    "# module_paper[\"image_augmentation-nas_svhn\"] = [\"AutoAugment: Learning Augmentation Policies from Data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noted that: imagenet-amoebanet_a_n18_f448-classification and imagenet-amoebanet_a_n18_f448-feature_vector have 2 references: regularized evolution for image classifier architecture search & learning transferable architectures for scalable image recognition.\n",
    "\n",
    "According to our implementation, the paper \"regularized ...\" will shown as \"arXiv:1802.01548\". That't not very important. We can always make the change manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_value_in_dict(key, dic):\n",
    "    try: \n",
    "        return dic[key]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def print_list_of_dict(ds, link1, link2, fileName = \"heheda.md\"):\n",
    "    ls = list(ds.keys())\n",
    "    ls.sort()\n",
    "    with open(fileName, \"w\") as fw:\n",
    "        counter = 0\n",
    "        for l in ls:\n",
    "            counter += 1\n",
    "            fw.write(f\"## {counter}. {l}\\n\")\n",
    "            fw.write(f\"* link: {find_value_in_dict(l, link1)}\\n\")\n",
    "            ds[l].sort()\n",
    "            fw.write(\"* items: \\n\")\n",
    "\n",
    "            for m in ds[l]:\n",
    "                fw.write(\"    * {} {}\\n\".format(m, find_value_in_dict(m, link2)))\n",
    "            fw.write(\"\\n\")\n",
    "\n",
    "print_list_of_dict(paper_module, paper_link, module_link, \"paper_module_list.md\")\n",
    "print_list_of_dict(module_paper, module_link, paper_link, \"module_paper_list.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_paper_module(ds, link1, link2, fileName = \"heheda.md\"):\n",
    "    ls = list(ds.keys())\n",
    "    ls.sort()\n",
    "    with open(fileName, \"w\") as fw:\n",
    "        counter = 0\n",
    "        for l in ls:\n",
    "            counter += 1\n",
    "            fw.write(f\"## {counter}. {l}\\n\")\n",
    "            fw.write(f\"* link: {find_value_in_dict(l, link1)}\\n\")\n",
    "            ds[l].sort()\n",
    "            fw.write(\"* modules: \\n\")\n",
    "\n",
    "            for m in ds[l]:\n",
    "                fw.write(f\"    * {m}\\n\")\n",
    "                fw.write(\"        * versions: {}\\n\".format(len(module_version[m])))\n",
    "                fw.write(\"        * link: {}\\n\".format(find_value_in_dict(m, link2)))\n",
    "                \n",
    "            fw.write(\"\\n\")\n",
    "print_paper_module(paper_module, paper_link, module_link, \"paper_module_list.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_module_paper(ds, link1, link2, fileName = \"heheda.md\"):\n",
    "    ls = list(ds.keys())\n",
    "    ls.sort()\n",
    "    with open(fileName, \"w\") as fw:\n",
    "        counter = 0\n",
    "        for l in ls:\n",
    "            counter += 1\n",
    "            fw.write(f\"## {counter}. {l}\\n\")\n",
    "            fw.write(f\"* link: {find_value_in_dict(l, link1)}\\n\")\n",
    "            fw.write(f\"* versions: {len(module_version[l])}\\n\")\n",
    "            ds[l].sort()\n",
    "            fw.write(\"* papers: \\n\")\n",
    "\n",
    "            for m in ds[l]:\n",
    "                fw.write(f\"    * {m} __link:__ {find_value_in_dict(m, link2)}\\n\")\n",
    "#                 fw.write(\"        * link: {}\\n\".format(find_value_in_dict(m, link2)))\n",
    "                \n",
    "            fw.write(\"\\n\")\n",
    "print_module_paper(module_paper, module_link, paper_link, \"module_paper_list.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(module_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 modules are not in there: \n",
    "\n",
    "#### 3 models don't have any reference info: \n",
    "\n",
    "(They don't clarify their references. Though their webpages look like the previous three, but I don't regard so for certain. )\n",
    "\n",
    "image_augmentation-crop_rotate_color 1 https://aihub.cloud.google.com/p/products%2Fcd9eda73-8cc4-4dbc-aa62-c97e98d657af\n",
    "\n",
    "image_augmentation-crop_color 1 https://aihub.cloud.google.com/p/products%2F83bb37d7-5060-4ba7-85e4-6c00a582e789\n",
    "\n",
    "image_augmentation-flipx_crop_rotate_color 1 https://aihub.cloud.google.com/p/products%2F2012b39c-9bd7-4670-bbc0-190c89405778\n",
    "\n",
    "#### 2 models clarify their referred stuff: \n",
    "faster_rcnn-openimages_v4-inception_resnet_v2 1 https://aihub.cloud.google.com/p/products%2F41b42dfa-e600-4a73-a425-7c5c4d511c3c\n",
    "* Only has some referred models and datasets: *FasterRCNN+InceptionResNetV2 network trained on Open Images V4*\n",
    "\n",
    "openimages_v4-ssd-mobilenet_v2 1 https://aihub.cloud.google.com/p/products%2F8c6878ba-d32d-411d-bac2-2f884b748c4f\n",
    "* Only has some referred models and datasets: *SSD+MobileNetV2 network trained on Open Images V4*\n",
    "\n",
    "#### 3 models don't have any reference info: \n",
    "\n",
    "ganeval-cifar10-convnet 1 https://aihub.cloud.google.com/p/products%2F110689d8-c594-49d6-aef4-9fc91c9c2ab4\n",
    "\n",
    "random-nnlm-en-dim50 1 https://aihub.cloud.google.com/p/products%2Fb0766fd5-82b5-48e3-9d6f-26c5d2601d5f\n",
    "\n",
    "random-nnlm-en-dim128 1 https://aihub.cloud.google.com/p/products%2Fa52e9466-f691-4f91-a62e-f6e39eb967f8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
