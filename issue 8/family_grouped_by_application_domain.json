{
    "image; augmentation": {
        "image_augmentation": {
            "family_description": "These modules performs dataset augmentation on images", 
            "image_augmentation-crop_color": {
                "application_domain": "image; augmentation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F83bb37d7-5060-4ba7-85e4-6c00a582e789", 
                "module_description": "cropping each input image (keeping at least 60% of the original area); distorting colors (brightness, hue, saturation, contrast)", 
                "references": null, 
                "version": 1
            }, 
            "image_augmentation-crop_rotate_color": {
                "application_domain": "image; augmentation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fcd9eda73-8cc4-4dbc-aa62-c97e98d657af", 
                "module_description": "cropping each input image (keeping at least 60% of the original area); rotating it (at most 30 degrees); distorting colors (brightness, hue, saturation, contrast)", 
                "references": null, 
                "version": 1
            }, 
            "image_augmentation-flipx_crop_rotate_color": {
                "application_domain": "image; augmentation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F2012b39c-9bd7-4670-bbc0-190c89405778", 
                "module_description": "flipping the input image (with 50% probability); cropping the image (keeping at least 60% of the original area); rotating it (at most 30 degrees); distorting colors (brightness, hue, saturation, contrast)", 
                "references": null, 
                "version": 1
            }
        }, 
        "image_augmentation-nas": {
            "family_description": "These modules performs dataset augmentation on images", 
            "image_augmentation-nas_cifar": {
                "application_domain": "image; augmentation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F822fa7f1-2207-4645-bcab-b7b916dae368", 
                "module_description": "use AutoAugment algorithm as augmentation policy; CIFAR", 
                "references": [
                    "AutoAugment: Learning Augmentation Policies from Data"
                ], 
                "version": 1
            }, 
            "image_augmentation-nas_imagenet": {
                "application_domain": "image; augmentation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fdaf153b0-cc1b-4f27-a037-5bd3d9fff224", 
                "module_description": "use AutoAugment algorithm as augmentation policy; imagenet", 
                "references": [
                    "AutoAugment: Learning Augmentation Policies from Data"
                ], 
                "version": 1
            }, 
            "image_augmentation-nas_svhn": {
                "application_domain": "image; augmentation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F5eef4456-8aa9-47d7-84db-00a7faa6662a", 
                "module_description": "use AutoAugment algorithm as augmentation policy; svhn", 
                "references": [
                    "AutoAugment: Learning Augmentation Policies from Data"
                ], 
                "version": 1
            }
        }
    }, 
    "image; classification": {
        "imagenet-amoebanet": {
            "family_description": "AmoebaNet is a family of convolutional neural networks for image classification; The architectures of its convolutional cells (or layers) have been found by an evolutionary architecture search in the NASNet search space", 
            "imagenet-amoebanet_a_n18_f448-classification": {
                "application_domain": "image; classification", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F689e685a-a1d1-494c-bec8-d7c4b1c6178a", 
                "module_description": "implementation of AmoebaNet-A; N = 18 Normal Cells; starting with F = 448 convolutional filters", 
                "references": [
                    "arXiv:1802.01548", 
                    "Learning Transferable Architectures for Scalable Image Recognition"
                ], 
                "version": 1
            }
        }, 
        "imagenet-inception_resnet_v2": {
            "family_description": "Inception ResNet V2 is a neural network architecture for image classification", 
            "imagenet-inception_resnet_v2-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F09208f7a-acf0-4ade-ab03-9b050e7a5a05", 
                "module_description": "1001 classes of the classification from the original training; size of the input images is height x width = 299 x 299 pixels by default", 
                "references": [
                    "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"
                ], 
                "version": 3
            }
        }, 
        "imagenet-inception_v1": {
            "family_description": "Inception V1 (a.k.a. GoogLeNet) is a neural network architecture for image classification", 
            "imagenet-inception_v1-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F7fecf7df-1263-4a82-bf39-ca3022b2d43e", 
                "module_description": "num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels", 
                "references": [
                    "Going deeper with convolutions"
                ], 
                "version": 3
            }
        }, 
        "imagenet-inception_v2": {
            "family_description": "Inception V2 is a neural network architecture for image classification; Inception V2 uses are more powerful architecture made possible by the use of batch normalization", 
            "imagenet-inception_v2-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F38e02e43-d7b5-4ff8-98c2-ec174f69e218", 
                "module_description": "num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels", 
                "references": [
                    "Going deeper with convolutions", 
                    "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
                ], 
                "version": 3
            }
        }, 
        "imagenet-inception_v3": {
            "family_description": "Inception V3 is a neural network architecture for image classification", 
            "imagenet-inception_v3-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fb96c2a1f-9304-49f2-8556-544cf5542ef0", 
                "module_description": "num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 299 x 299 pixels", 
                "references": [
                    "Rethinking the Inception Architecture for Computer Vision"
                ], 
                "version": 3
            }
        }, 
        "imagenet-mobilenet_v1": {
            "family_description": "MobileNet V1 is a family of neural network architectures for efficient on-device image classification; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed", 
            "imagenet-mobilenet_v1_025_128-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F2e357bad-a058-4de8-b239-7a5f1058dabb", 
                "module_description": "TF-Slim implementation of mobilenet_v1_025 with a depth multiplier of 0.25 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 128 x 128 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_160-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Faf498bc9-9b4f-4919-ae40-193d3e6f7203", 
                "module_description": "TF-Slim implementation of mobilenet_v1_025 with a depth multiplier of 0.25 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 160 x 160 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_192-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fb23c2250-dee1-4459-9ac6-8c327ca92fdc", 
                "module_description": "TF-Slim implementation of mobilenet_v1_025 with a depth multiplier of 0.25 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 192 x 192 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_224-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F93839d48-c253-4ea8-83a3-d54547092bfb", 
                "module_description": "TF-Slim implementation of mobilenet_v1_025 with a depth multiplier of 0.25 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_128-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F80eba1e9-cab6-4bc4-8618-59ed39f539fb", 
                "module_description": "TF-Slim implementation of mobilenet_v1_050 with a depth multiplier of 0.5 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 128 x 128 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_160-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F3b2b0bb1-3328-4391-999d-99da1adc16f3", 
                "module_description": "TF-Slim implementation of mobilenet_v1_050 with a depth multiplier of 0.5 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 160 x 160 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_192-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fc4b35f6b-82d8-4e76-aba2-9f9e18fe6c06", 
                "module_description": "TF-Slim implementation of mobilenet_v1_050 with a depth multiplier of 0.5 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 192 x 192 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_224-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F7578eb13-6458-4796-80fa-86d68e4cc092", 
                "module_description": "TF-Slim implementation of mobilenet_v1_050 with a depth multiplier of 0.5 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_128-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F34c8ee65-d4fe-4234-a3f4-7d18c404c479", 
                "module_description": "TF-Slim implementation of mobilenet_v1_075 with a depth multiplier of 0.75 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 128 x 128 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_160-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Feb982d0a-f086-4408-875c-dce5d8272fab", 
                "module_description": "TF-Slim implementation of mobilenet_v1_075 with a depth multiplier of 0.75 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 160 x 160 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_192-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F2d0403f6-c7bf-4f87-a8aa-fd04c96d17c3", 
                "module_description": "TF-Slim implementation of mobilenet_v1_075 with a depth multiplier of 0.75 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 192 x 192 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_224-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fe3a910da-c3bd-4bc8-94fb-f941e9fa0e50", 
                "module_description": "TF-Slim implementation of mobilenet_v1_075 with a depth multiplier of 0.75 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_128-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Ffb69f59b-0dfa-4fda-a8f0-66b0e4c9fd7b", 
                "module_description": "TF-Slim implementation of mobilenet_v1 with a depth multiplier of 1.0 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 128 x 128 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_160-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fc7a49fbc-5f83-49a4-beb3-a726efc7cc69", 
                "module_description": "TF-Slim implementation of mobilenet_v1 with a depth multiplier of 1.0 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 160 x 160 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_192-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Ffcd3da5a-fe3d-4a6a-a56f-f476f7449bb4", 
                "module_description": "TF-Slim implementation of mobilenet_v1 with a depth multiplier of 1.0 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 192 x 192 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_224-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fff23bf1c-5c35-40b0-95cb-86b7cc6f15e6", 
                "module_description": "TF-Slim implementation of mobilenet_v1 with a depth multiplier of 1.0 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }
        }, 
        "imagenet-mobilenet_v2": {
            "family_description": "MobileNet V2 is a family of neural network architectures for efficient on-device image classification and related tasks; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed", 
            "imagenet-mobilenet_v2_035_128-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F13041a34-cc37-4e4f-bd04-ac0bd5156e68", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.35 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_035_160-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fe7ca0182-26a9-431b-897f-48e5cf9f1c1a", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.35 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_035_192-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F613b1e96-3dcf-4a97-93aa-b491d164c3d9", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.35 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_035_224-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F1b4c3f06-200f-4116-a5e3-4cefc5799c63", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.35 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_035_96-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F7097ed9b-8957-4b28-a2a9-4bbf22bd0462", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.35 and an input size of 96x96 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 96 x 96 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_050_128-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fed2f3c46-8251-4160-abce-c9e1e05e6ef1", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.5 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_050_160-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fbb1f7123-166f-4fa5-b71d-0d36045a82f8", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.5 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_050_192-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F98d0512a-3f5a-46ac-b4d9-0e923f9462eb", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.5 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_050_224-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F40682478-2573-40f6-bd1f-4bd3882e9dea", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.5 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_050_96-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fb8f781b4-7733-4238-a89a-b94c5903d44e", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.5 and an input size of 96x96 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 96 x 96 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_075_128-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F401734b5-e64d-4a7f-8044-25c8cf8a975a", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.75 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_075_160-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F15f65f21-50d2-4869-baa0-cfced0e744c2", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.75 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_075_192-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F0396a9ca-de5c-4a7f-9aa6-4f04aaff118d", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.75 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_075_224-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fcb908573-5a9d-44e0-a406-08c6e847d898", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.75 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_075_96-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fe008f1ac-7f58-4ddf-bcc2-aaa396c9f3fc", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.75 and an input size of 96x96 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 96 x 96 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_100_128-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fa99e7ea9-4cfb-4789-89aa-375c94c6dcdb", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_100_160-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F582ec440-e350-4611-9a72-3bb3b949f457", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_100_192-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fc122c154-deed-42e6-be7f-558615fa50a6", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_100_224-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F5ea70ed9-9bd7-41e5-898f-e1509e8793bd", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_100_96-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F48b59149-3590-4994-9256-3fe434a827b3", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 96x96 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 96 x 96 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_130_224-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F87265e6e-af6c-4071-957f-01b8c9afc719", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.3 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_140_224-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Ff4500342-7736-49e9-93b2-1535b377109f", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.4 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }
        }, 
        "imagenet-nasnet": {
            "family_description": "NASNet-A is a family of convolutional neural networks for image classification. The architecture of its convolutional cells (or layers) has been found by Neural Architecture Search (NAS); NASNets come in various sizes", 
            "imagenet-nasnet_large-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F2f3d31d4-66ef-4fb9-8d48-075f462ffe9b", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation nasnet_large of NASNet-A for ImageNet that uses 18 Normal Cells, starting with 168 convolutional filters (after the \"ImageNet stem\"). It has an input size of 331x331 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 331 x 331 pixels", 
                "references": [
                    "Neural Architecture Search with Reinforcement Learning", 
                    "Learning Transferable Architectures for Scalable Image Recognition"
                ], 
                "version": 3
            }, 
            "imagenet-nasnet_mobile-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F368aeea6-767b-43a7-a531-13a95b5bd895", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation nasnet_mobile of NASNet-A for ImageNet that uses 12 Normal Cells, starting with 44 convolutional filters (after the \"ImageNet stem\"). It has an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Neural Architecture Search with Reinforcement Learning", 
                    "Learning Transferable Architectures for Scalable Image Recognition"
                ], 
                "version": 3
            }
        }, 
        "imagenet-pnasnet": {
            "family_description": "PNASNet-5 is a family of convolutional neural networks for image classification; The architecture of its convolutional cells (or layers) has been found by Progressive Neural Architecture Search; PNASNet reuses several techniques from is precursor NASNet, including regularization by path dropout", 
            "imagenet-pnasnet_large-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F46974972-0006-4fd5-8dc8-2f68ab53a378", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation pnasnet_large of PNASNet-5 for ImageNet that uses 12 cells (plus 2 for the \"ImageNet stem\"), starting with 216 convolutional filters (after the stem). It has an input size of 331x331 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 331 x 331 pixels", 
                "references": [
                    "Progressive Neural Architecture Search", 
                    "Learning Transferable Architectures for Scalable Image Recognition"
                ], 
                "version": 3
            }
        }, 
        "imagenet-resnet_v1": {
            "family_description": "ResNet (later renamed ResNet V1) is a family of network architectures for image classification with a variable number of layers", 
            "imagenet-resnet_v1_101-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Ff3cabfdf-b54d-47b4-8e89-036ef39658fa", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v1_101 with 101 layers; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition"
                ], 
                "version": 3
            }, 
            "imagenet-resnet_v1_152-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F76a23fa7-e450-46c0-8a7a-deb0b6d0d8af", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v1_152 with 152 layers; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition"
                ], 
                "version": 3
            }, 
            "imagenet-resnet_v1_50-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F67033c91-b2ee-43cc-9c7a-fbe0550b88d8", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v1_50 with 50 layers; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition"
                ], 
                "version": 3
            }
        }, 
        "imagenet-resnet_v2": {
            "family_description": "ResNet V2 is a family of network architectures for image classification with a variable number of layers. It builds on the ResNet architecture; The key difference compared to ResNet V1 is the use of batch normalization before every weight layer", 
            "imagenet-resnet_v2_101-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F4d580b21-7dce-4d2a-a240-f61cb76a75cb", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v2_101 with 101 layers; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition", 
                    "Identity Mappings in Deep Residual Networks"
                ], 
                "version": 3
            }, 
            "imagenet-resnet_v2_152-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F30e3d1be-cece-4c09-9a7c-03c6841a5f41", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v2_152 with 152 layers; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition", 
                    "Identity Mappings in Deep Residual Networks"
                ], 
                "version": 3
            }, 
            "imagenet-resnet_v2_50-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F9f12fca3-c88a-4548-9c29-d875af05f22b", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v2_50 with 50 layers; num_classes = 1001 classes of the classification from the original training; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition", 
                    "Identity Mappings in Deep Residual Networks"
                ], 
                "version": 3
            }
        }, 
        "quantops": {
            "family_description": "MobileNet V1 is a family of neural network architectures for efficient on-device image classification; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed; The implementation is instrumented for quantization", 
            "imagenet-mobilenet_v1_025_128-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fc66937a1-5ef0-4b23-aa9d-7c49097d19c5", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_025, instrumented for quantization, with a depth multiplier of 0.25 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_160-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fb2c8e4bf-a4aa-404c-8cae-525a9944182a", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_025, instrumented for quantization, with a depth multiplier of 0.25 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_192-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F8b75406e-47e4-4261-bd0f-9fdf32ee329d", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_025, instrumented for quantization, with a depth multiplier of 0.25 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_224-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F848e4116-1803-49ba-9c74-23365d448428", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_025, instrumented for quantization, with a depth multiplier of 0.25 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_128-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F0dc0d585-1ff5-4501-b9ce-5a0dd183d41b", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_050, instrumented for quantization, with a depth multiplier of 0.5 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_160-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fefbecded-bb7e-4d95-a784-35c09efb84fc", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_050, instrumented for quantization, with a depth multiplier of 0.5 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_192-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F558058fe-0d24-4bdc-a4b5-739ca7adb7b5", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_050, instrumented for quantization, with a depth multiplier of 0.5 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_224-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F711fda69-6f28-40e7-84bc-bd9f811a1eb8", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_050, instrumented for quantization, with a depth multiplier of 0.5 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_128-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fc2480cdc-afac-4f50-b222-0988427d84ed", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_075, instrumented for quantization, with a depth multiplier of 0.75 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_160-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F83a5c631-0c56-40e7-93f2-d34831f317c1", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_075, instrumented for quantization, with a depth multiplier of 0.75 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_192-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fa455648c-ccd4-46ad-a743-fa6f82d96a5c", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_075, instrumented for quantization, with a depth multiplier of 0.75 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_224-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F4a5326c3-a371-4609-b8c7-05d10719b171", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_075, instrumented for quantization, with a depth multiplier of 0.75 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_128-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F527e439e-fc16-4419-bdbb-caf7f247009e", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1, instrumented for quantization, with a depth multiplier of 1.0 and an input size of 128x128 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_160-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Ffe2a7857-566b-45b9-bc85-22e2f90f8904", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1, instrumented for quantization, with a depth multiplier of 1.0 and an input size of 160x160 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_192-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F99959f36-5676-4fcf-af54-42adc89d2cce", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1, instrumented for quantization, with a depth multiplier of 1.0 and an input size of 192x192 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_224-quantops-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F0940e0bd-5d4c-44ae-b51b-f48a93109166", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1, instrumented for quantization, with a depth multiplier of 1.0 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }
        }, 
        "tf2-preview-inception_v3": {
            "family_description": "SavedModel 2.0 format; help preview TensorFlow 2.0 functionality; Inception V3 is a neural network architecture for image classification", 
            "tf2-preview-inception_v3-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed missing default <code>trainable=False</code>.</li>\n<li>Fixed broken regularization_losses.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Provides proper names for variables, fixing crash in <code>Model.save()</code>\n    (<a href=\"https://github.com/tensorflow/hub/issues/287\">GitHub issue #287</a>).</li>\n</ul>\n<h4>Version 4</h4>\n<ul>\n<li>Adds back missing update ops for batch norm that were lost in version 3,\n    (<a href=\"https://github.com/tensorflow/hub/issues/304\">GitHub issue #304</a>).</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fe58473e0-fbf7-40bd-9cea-917e3471ce2b", 
                "module_description": "num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 299 x 299 pixels", 
                "references": [
                    "Rethinking the Inception Architecture for Computer Vision"
                ], 
                "version": 4
            }
        }, 
        "tf2-preview-mobilenet_v2": {
            "family_description": "SavedModel 2.0 format; help preview TensorFlow 2.0 functionality; MobileNet V2 is a family of neural network architectures for efficient on-device image classification and related tasks; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed", 
            "tf2-preview-mobilenet_v2-classification": {
                "application_domain": "image; classification", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed missing default <code>trainable=False</code>.</li>\n<li>Fixed broken regularization_losses.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Provides proper names for variables, fixing crash in <code>Model.save()</code>\n    (<a href=\"https://github.com/tensorflow/hub/issues/287\">GitHub issue #287</a>).</li>\n</ul>\n<h4>Version 4</h4>\n<ul>\n<li>Adds back missing update ops for batch norm that were lost in version 3,\n    (<a href=\"https://github.com/tensorflow/hub/issues/304\">GitHub issue #304</a>).</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fea5d2e6b-5ebe-4c14-a38a-5b9c10f542d2", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 224x224 pixels; num_classes = 1001 classes of the classification from the original training; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 4
            }
        }
    }, 
    "image; feature_vector": {
        "imagenet-amoebanet": {
            "family_description": "AmoebaNet is a family of convolutional neural networks for image classification; The architectures of its convolutional cells (or layers) have been found by an evolutionary architecture search in the NASNet search space", 
            "imagenet-amoebanet_a_n18_f448-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F20edefb4-125e-45ac-9639-9786d20007cc", 
                "module_description": "implementation of AmoebaNet-A; N = 18 Normal Cells; starting with F = 448 convolutional filters", 
                "references": [
                    "arXiv:1802.01548", 
                    "Learning Transferable Architectures for Scalable Image Recognition"
                ], 
                "version": 1
            }
        }, 
        "imagenet-inception_resnet_v2": {
            "family_description": "Inception ResNet V2 is a neural network architecture for image classification", 
            "imagenet-inception_resnet_v2-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F462e23f3-f3c3-4c85-8cdf-7ee1f49524fd", 
                "module_description": "feature vector of size num_features = 1536; input images is height x width = 299 x 299 pixels by default", 
                "references": [
                    "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"
                ], 
                "version": 3
            }
        }, 
        "imagenet-inception_v1": {
            "family_description": "Inception V1 (a.k.a. GoogLeNet) is a neural network architecture for image classification", 
            "imagenet-inception_v1-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F1d3a69a4-7546-436e-8e4d-98170906cd44", 
                "module_description": "feature vector of size num_features = 1024; size of the input images is height x width = 224 x 224 pixels", 
                "references": [
                    "Going deeper with convolutions"
                ], 
                "version": 3
            }
        }, 
        "imagenet-inception_v2": {
            "family_description": "Inception V2 is a neural network architecture for image classification; Inception V2 uses are more powerful architecture made possible by the use of batch normalization", 
            "imagenet-inception_v2-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F3fdd6238-9978-48cf-b5ed-4f5c0b2dbec8", 
                "module_description": "feature vector of size num_features = 1024; size of the input images is height x width = 224 x 224 pixels", 
                "references": [
                    "Going deeper with convolutions", 
                    "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
                ], 
                "version": 3
            }
        }, 
        "imagenet-inception_v3": {
            "family_description": "Inception V3 is a neural network architecture for image classification", 
            "imagenet-inception_v3-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F05659cd0-245d-4a76-9a1a-9fc59c56b1bc", 
                "module_description": "feature vector of size num_features = 2048; size of the input images is height x width = 299 x 299 pixels", 
                "references": [
                    "Rethinking the Inception Architecture for Computer Vision"
                ], 
                "version": 3
            }
        }, 
        "imagenet-mobilenet_v1": {
            "family_description": "MobileNet V1 is a family of neural network architectures for efficient on-device image classification; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed", 
            "imagenet-mobilenet_v1_025_128-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fbaf42351-a9d6-421d-b521-f01b51494322", 
                "module_description": "TF-Slim implementation of mobilenet_v1_025 with a depth multiplier of 0.25 and an input size of 128x128 pixels; feature vector of size num_features = 256; size of the input images is height x width = 128 x 128 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_160-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F256a905e-5af8-48e6-8a92-560ba4257176", 
                "module_description": "TF-Slim implementation of mobilenet_v1_025 with a depth multiplier of 0.25 and an input size of 160x160 pixels; feature vector of size num_features = 256; size of the input images is height x width = 160 x 160 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_192-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fac545e36-64f2-4c3a-bffc-7bd059cc4e1c", 
                "module_description": "TF-Slim implementation of mobilenet_v1_025 with a depth multiplier of 0.25 and an input size of 192x192 pixels; feature vector of size num_features = 256; size of the input images is height x width = 192 x 192 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_224-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F8efa0299-dd99-44f2-839f-9ec99fd11d6d", 
                "module_description": "TF-Slim implementation of mobilenet_v1_025 with a depth multiplier of 0.25 and an input size of 224x224 pixels; feature vector of size num_features = 256; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_128-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Ffca984ef-e43f-40fa-8b51-a08cbd609757", 
                "module_description": "TF-Slim implementation of mobilenet_v1_050 with a depth multiplier of 0.5 and an input size of 128x128 pixels; feature vector of size num_features = 512; size of the input images is height x width = 128 x 128 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_160-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F94e358ac-4513-426d-9cb0-168a4c985ba8", 
                "module_description": "TF-Slim implementation of mobilenet_v1_050 with a depth multiplier of 0.5 and an input size of 160x160 pixels; feature vector of size num_features = 512; size of the input images is height x width = 160 x 160 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_192-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fc66d9a19-c102-4ed9-a913-cf4165caed13", 
                "module_description": "TF-Slim implementation of mobilenet_v1_050 with a depth multiplier of 0.5 and an input size of 192x192 pixels; feature vector of size num_features = 512; size of the input images is height x width = 192 x 192 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_224-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F8c43f01d-bf62-4e71-9526-e05d6ed60179", 
                "module_description": "TF-Slim implementation of mobilenet_v1_050 with a depth multiplier of 0.5 and an input size of 224x224 pixels; feature vector of size num_features = 512; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_128-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fdb4f3daf-ccef-4768-9f55-84f50bd13274", 
                "module_description": "TF-Slim implementation of mobilenet_v1_075 with a depth multiplier of 0.75 and an input size of 128x128 pixels; feature vector of size num_features = 768; size of the input images is height x width = 128 x 128 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_160-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Ffe8d5ee8-a85d-4412-87db-5b56a0f72315", 
                "module_description": "TF-Slim implementation of mobilenet_v1_075 with a depth multiplier of 0.75 and an input size of 160x160 pixels; feature vector of size num_features = 768; size of the input images is height x width = 160 x 160 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_192-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fb35bd7a7-18c4-4a92-9731-f50f6205959c", 
                "module_description": "TF-Slim implementation of mobilenet_v1_075 with a depth multiplier of 0.75 and an input size of 192x192 pixels; feature vector of size num_features = 768; size of the input images is height x width = 192 x 192 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_224-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Feec28b3b-0e72-4280-ac48-56e159dc2b36", 
                "module_description": "TF-Slim implementation of mobilenet_v1_075 with a depth multiplier of 0.75 and an input size of 224x224 pixels; feature vector of size num_features = 768; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_128-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F37f37ece-e98b-4283-a370-462e1392c60d", 
                "module_description": "TF-Slim implementation of mobilenet_v1 with a depth multiplier of 1.0 and an input size of 128x128 pixels; feature vector of size num_features = 1024; size of the input images is height x width = 128 x 128 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_160-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fc071f984-95e4-45fa-9f00-285771a9d590", 
                "module_description": "TF-Slim implementation of mobilenet_v1 with a depth multiplier of 1.0 and an input size of 160x160 pixels; feature vector of size num_features = 1024; size of the input images is height x width = 160 x 160 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_192-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F20a94dd6-8ea0-45df-a3eb-e0b7059ab706", 
                "module_description": "TF-Slim implementation of mobilenet_v1 with a depth multiplier of 1.0 and an input size of 192x192 pixels; feature vector of size num_features = 1024; size of the input images is height x width = 192 x 192 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_224-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F8eca3922-f765-4897-97d3-8a99e8ff14f5", 
                "module_description": "TF-Slim implementation of mobilenet_v1 with a depth multiplier of 1.0 and an input size of 224x224 pixels; feature vector of size num_features = 1024; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }
        }, 
        "imagenet-mobilenet_v2": {
            "family_description": "MobileNet V2 is a family of neural network architectures for efficient on-device image classification and related tasks; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed", 
            "imagenet-mobilenet_v2_035_128-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F41c4959a-226c-4be8-807d-561a836966a2", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.35 and an input size of 128x128 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_035_128/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_035_128/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.35_128/mobilenet_v2_0.35_128.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_035_128/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_035_160-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fd3e7b6ae-80dd-4bee-97a2-f91fc2d21d6c", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.35 and an input size of 160x160 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_035_160/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_035_160/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.35_160/mobilenet_v2_0.35_160.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_035_160/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_035_192-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F16532d4b-ce52-4b18-adad-50416a10922e", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.35 and an input size of 192x192 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_035_192/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_035_192/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.35_192/mobilenet_v2_0.35_192.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_035_192/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_035_224-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F8bc76c00-5f99-4894-92c6-05de3148e7ac", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.35 and an input size of 224x224 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_035_224/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_035_224/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.35_224/mobilenet_v2_0.35_224.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_035_224/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_035_96-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F9939bd82-37ac-4fc9-809d-d61df1891908", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.35 and an input size of 96x96 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_035_96/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_035_96/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.35_96/mobilenet_v2_0.35_96.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_035_96/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 96 x 96 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_050_128-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F916a7fd4-bbac-4e6a-91f6-9e5af3e4afee", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.5 and an input size of 128x128 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_050_128/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_050_128/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.5_128/mobilenet_v2_0.5_128.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_050_128/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_050_160-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F49b83aab-12dd-413e-928f-f85cf8b0ca3c", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.5 and an input size of 160x160 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_050_160/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_050_160/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.5_160/mobilenet_v2_0.5_160.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_050_160/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_050_192-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Ff5db7910-0fcb-44b7-9fb6-d153ee00b8e7", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.5 and an input size of 192x192 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_050_192/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_050_192/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.5_192/mobilenet_v2_0.5_192.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_050_192/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_050_224-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F202d7c3a-7d04-4bc9-a72e-42dcced3b0ae", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.5 and an input size of 224x224 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_050_224/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_050_224/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.5_224/mobilenet_v2_0.5_224.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_050_224/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_050_96-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Feb7c9518-fb2e-4dbf-934c-95d2af6f257a", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.5 and an input size of 96x96 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_050_96/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_050_96/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.5_96/mobilenet_v2_0.5_96.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_050_96/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 96 x 96 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_075_128-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F88a471da-49a6-46c8-85c3-aaf5c4711d5b", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.75 and an input size of 128x128 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_075_128/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_075_128/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.75_128/mobilenet_v2_0.75_128.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_075_128/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_075_160-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F85a2d4e2-3a0d-49c2-9365-ae12d71b0dad", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.75 and an input size of 160x160 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_075_160/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_075_160/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.75_160/mobilenet_v2_0.75_160.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_075_160/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_075_192-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F5ca55fc5-e5ce-44bd-a562-d56ce8dc3e4a", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.75 and an input size of 192x192 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_075_192/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_075_192/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.75_192/mobilenet_v2_0.75_192.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_075_192/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_075_224-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F8bd7f50f-ac56-4f7d-848c-398d316c9838", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.75 and an input size of 224x224 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_075_224/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_075_224/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.75_224/mobilenet_v2_0.75_224.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_075_224/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_075_96-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F8c7ef4e2-5856-4f0b-b00c-992c4ec9df8f", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 0.75 and an input size of 96x96 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_075_96/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_075_96/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_0.75_96/mobilenet_v2_0.75_96.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_075_96/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 96 x 96 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_100_128-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F9ee55fc9-cb7f-4596-9d71-1d15369a60f1", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 128x128 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_100_128/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_100_128/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_1.0_128/mobilenet_v2_1.0_128.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_128/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_100_160-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F02b5212f-978b-4038-9824-a039cc948f0f", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 160x160 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_100_160/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_100_160/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_1.0_160/mobilenet_v2_1.0_160.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_160/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_100_192-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fcdf6d95f-fd41-41e4-8cba-0f5762aa4649", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 192x192 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_100_192/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_100_192/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_1.0_192/mobilenet_v2_1.0_192.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_192/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_100_224-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F4c463130-1956-4a25-bed2-cace34d3ff00", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 224x224 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_100_224/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_100_96-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F4ef65e19-ef56-42a4-87ca-5e4d47bf6027", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 96x96 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_100_96/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_100_96/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_1.0_96/mobilenet_v2_1.0_96.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_96/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1280; size of the input images is fixed to height x width = 96 x 96 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_130_224-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F8db8dfc9-9a17-4fa2-b82f-e2844a93baf6", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.3 and an input size of 224x224 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_130_224/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_1.3_224/mobilenet_v2_1.3_224.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1664; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v2_140_224-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F0d6b8331-887b-455f-962f-5af5b868002f", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.4 and an input size of 224x224 pixels; feature vector).  The module contains a trained instance of the network, packaged to get [feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v2_140_224/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/3) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_1.4_224/mobilenet_v2_1.4_224.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1792; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 3
            }
        }, 
        "imagenet-nasnet": {
            "family_description": "NASNet-A is a family of convolutional neural networks for image classification. The architecture of its convolutional cells (or layers) has been found by Neural Architecture Search (NAS); NASNets come in various sizes", 
            "imagenet-nasnet_large-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fe3caa896-0626-4fc0-b8ad-2a28bdd694eb", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation nasnet_large of NASNet-A for ImageNet that uses 18 Normal Cells, starting with 168 convolutional filters (after the \"ImageNet stem\"). It has an input size of 331x331 pixels; feature vector of size num_features = 4032; size of the input images is fixed to height x width = 331 x 331 pixels", 
                "references": [
                    "Neural Architecture Search with Reinforcement Learning", 
                    "Learning Transferable Architectures for Scalable Image Recognition"
                ], 
                "version": 3
            }, 
            "imagenet-nasnet_mobile-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F2ecbe2f7-e391-426f-b591-54978231ff14", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation nasnet_mobile of NASNet-A for ImageNet that uses 12 Normal Cells, starting with 44 convolutional filters (after the \"ImageNet stem\"). It has an input size of 224x224 pixels; feature vector of size num_features = 1056; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Neural Architecture Search with Reinforcement Learning", 
                    "Learning Transferable Architectures for Scalable Image Recognition"
                ], 
                "version": 3
            }
        }, 
        "imagenet-pnasnet": {
            "family_description": "PNASNet-5 is a family of convolutional neural networks for image classification; The architecture of its convolutional cells (or layers) has been found by Progressive Neural Architecture Search; PNASNet reuses several techniques from is precursor NASNet, including regularization by path dropout", 
            "imagenet-pnasnet_large-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed broken UPDATE_OPS for fine-tuning,\n    <a href=\"https://github.com/tensorflow/hub/issues/86\">GitHub issue 86</a>.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F352a1ce8-4289-4130-8e5b-0b6a978a3691", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation pnasnet_large of PNASNet-5 for ImageNet that uses 12 cells (plus 2 for the \"ImageNet stem\"), starting with 216 convolutional filters (after the stem). It has an input size of 331x331 pixels; feature vector of size num_features = 4320; size of the input images is fixed to height x width = 331 x 331 pixels", 
                "references": [
                    "Progressive Neural Architecture Search", 
                    "Learning Transferable Architectures for Scalable Image Recognition"
                ], 
                "version": 3
            }
        }, 
        "imagenet-resnet_v1": {
            "family_description": "ResNet (later renamed ResNet V1) is a family of network architectures for image classification with a variable number of layers", 
            "imagenet-resnet_v1_101-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Faf07f374-03bf-487e-bc2d-6e5596d9213f", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v1_101 with 101 layers; feature vector of size num_features = 2048; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition"
                ], 
                "version": 3
            }, 
            "imagenet-resnet_v1_152-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F64fa952d-d1e1-4ba6-9b81-a061c33a8ad7", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v1_152 with 152 layers; feature vector of size num_features = 2048; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition"
                ], 
                "version": 3
            }, 
            "imagenet-resnet_v1_50-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fb4e108ed-8450-426f-94fc-bffe396e4a1d", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v1_50 with 50 layers; feature vector of size num_features = 2048; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition"
                ], 
                "version": 3
            }
        }, 
        "imagenet-resnet_v2": {
            "family_description": "ResNet V2 is a family of network architectures for image classification with a variable number of layers. It builds on the ResNet architecture; The key difference compared to ResNet V1 is the use of batch normalization before every weight layer", 
            "imagenet-resnet_v2_101-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F4dd7a845-66db-4ac0-a4b6-b1933b1dc281", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v2_101 with 101 layers; feature vector of size num_features = 2048; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition", 
                    "Identity Mappings in Deep Residual Networks"
                ], 
                "version": 3
            }, 
            "imagenet-resnet_v2_152-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fdb11b3e9-446e-455a-877c-9d812fe96585", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v2_152 with 152 layers; feature vector of size num_features = 2048; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition", 
                    "Identity Mappings in Deep Residual Networks"
                ], 
                "version": 3
            }, 
            "imagenet-resnet_v2_50-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Support for variable input size.</li>\n<li>Fine-tuning: change default batch norm momentum to 0.99 and\n    make it configurable.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fa350eeeb-87ff-4335-9dc3-b2b7ec1db794", 
                "module_description": "This TF-Hub module uses the TF-Slim implementation of resnet_v2_50 with 50 layers; feature vector of size num_features = 2048; size of the input images is height x width = 224 x 224 pixels by default", 
                "references": [
                    "Deep Residual Learning for Image Recognition", 
                    "Identity Mappings in Deep Residual Networks"
                ], 
                "version": 3
            }
        }, 
        "quantops": {
            "family_description": "MobileNet V1 is a family of neural network architectures for efficient on-device image classification; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed; The implementation is instrumented for quantization", 
            "imagenet-mobilenet_v1_025_128-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Ff5f87ef8-0b36-4d85-9664-8fc6a7e4fc0f", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_025, instrumented for quantization, with a depth multiplier of 0.25 and an input size of 128x128 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_025_128/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_025_128/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.25_128_quant/mobilenet_v1_0.25_128_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_025_128/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 256; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_160-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F7d9362ba-5f53-44b2-acdc-131691e23e29", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_025, instrumented for quantization, with a depth multiplier of 0.25 and an input size of 160x160 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_025_160/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_025_160/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.25_160_quant/mobilenet_v1_0.25_160_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_025_160/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 256; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_192-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F4e39382a-955f-438e-b5bd-9c45df0c50d3", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_025, instrumented for quantization, with a depth multiplier of 0.25 and an input size of 192x192 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_025_192/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_025_192/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.25_192_quant/mobilenet_v1_0.25_192_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_025_192/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 256; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_025_224-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F9ad83825-4d7d-4729-9aba-e1657cf82955", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_025, instrumented for quantization, with a depth multiplier of 0.25 and an input size of 224x224 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_025_224/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_025_224/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.25_224_quant/mobilenet_v1_0.25_224_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_025_224/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 256; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_128-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F5ca67ec2-8aae-43a1-aa60-c14a7469d50c", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_050, instrumented for quantization, with a depth multiplier of 0.5 and an input size of 128x128 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_050_128/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_050_128/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.5_128_quant/mobilenet_v1_0.5_128_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_050_128/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 512; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_160-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Ff71d5c2a-fc77-44cd-9ec7-b17a68595ac5", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_050, instrumented for quantization, with a depth multiplier of 0.5 and an input size of 160x160 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_050_160/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.5_160_quant/mobilenet_v1_0.5_160_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 512; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_192-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F1e335893-c2c1-443f-be23-04240fbc7839", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_050, instrumented for quantization, with a depth multiplier of 0.5 and an input size of 192x192 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_050_192/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_050_192/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.5_192_quant/mobilenet_v1_0.5_192_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_050_192/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 512; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_050_224-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fbe53911c-ce9f-4bc0-b229-3783fd7eb516", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_050, instrumented for quantization, with a depth multiplier of 0.5 and an input size of 224x224 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_050_224/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_050_224/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.5_224_quant/mobilenet_v1_0.5_224_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_050_224/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 512; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_128-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fb4168dee-fc60-4983-9342-82df37168fef", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_075, instrumented for quantization, with a depth multiplier of 0.75 and an input size of 128x128 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_075_128/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_075_128/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.75_128_quant/mobilenet_v1_0.75_128_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_075_128/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 768; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_160-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F30ed3255-649b-4d5d-8eab-4e2d1ea20a90", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_075, instrumented for quantization, with a depth multiplier of 0.75 and an input size of 160x160 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_075_160/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_075_160/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.75_160_quant/mobilenet_v1_0.75_160_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_075_160/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 768; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_192-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F189a3dd7-35ea-4a53-ab2c-746396b884cb", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_075, instrumented for quantization, with a depth multiplier of 0.75 and an input size of 192x192 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_075_192/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_075_192/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.75_192_quant/mobilenet_v1_0.75_192_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_075_192/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 768; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_075_224-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fa41b9636-3ef7-421a-8594-d46037d1f0d6", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1_075, instrumented for quantization, with a depth multiplier of 0.75 and an input size of 224x224 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_075_224/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_075_224/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_0.75_224_quant/mobilenet_v1_0.75_224_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_075_224/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 768; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_128-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fffd67803-8128-49d5-a310-896ccbfda23b", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1, instrumented for quantization, with a depth multiplier of 1.0 and an input size of 128x128 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_100_128/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_100_128/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_1.0_128_quant/mobilenet_v1_1.0_128_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_100_128/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1024; size of the input images is fixed to height x width = 128 x 128 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_160-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2F16f425f5-53ad-43a4-9fb2-b454e7dd611d", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1, instrumented for quantization, with a depth multiplier of 1.0 and an input size of 160x160 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_100_160/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_100_160/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_1.0_160_quant/mobilenet_v1_1.0_160_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_100_160/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1024; size of the input images is fixed to height x width = 160 x 160 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_192-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fe519e11e-8155-46b6-892e-86e39a9af2cd", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1, instrumented for quantization, with a depth multiplier of 1.0 and an input size of 192x192 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_100_192/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_100_192/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_1.0_192_quant/mobilenet_v1_1.0_192_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_100_192/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1024; size of the input images is fixed to height x width = 192 x 192 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }, 
            "imagenet-mobilenet_v1_100_224-quantops-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Avoids crash in some TFLite/toco versions\n    (<a href=\"https://github.com/tensorflow/hub/issues/109\">GitHub issue 109</a>)\n    by overestimating quantization boundaries on input image by 0.05%.</li>\n<li>Requires PIP package <code>tensorflow-hub&gt;=0.2.0</code>.</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Faaf6a352-ad6b-4769-a4cc-f237465fd989", 
                "module_description": "TF-Slim implementation of mobilenet_v1_v1, instrumented for quantization, with a depth multiplier of 1.0 and an input size of 224x224 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/imagenet/mobilenet_v1_100_224/quantops/classification/3](https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/quantops/classification/3) instead.   ## Quantization  This module is meant for use in models whose weights will be quantized to uint8 by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) for deployment to mobile devices.  The trained weights of this module are shipped as float32 numbers, but its graph has been augmented by tf.contrib.quantize with extra ops that simulate the effect of quantization already during training, so that the model can adjust to it.  ## Training  The checkpoint exported into this module was mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224_quant/mobilenet_v1_1.0_224_quant.ckpt downloaded from [MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\"), with simulated quantization.  ## Usage  This module implements the common signature for computing [image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). It can be used like  python module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/quantops/feature_vector/3\") height, width = hub.get_expected_image_size(module) images = ...  # A batch of images with shape [batch_size, height, width, 3]. features = module(images)  # Features with shape [batch_size, num_features].   ...or using the signature name image_feature_vector. The output for each image in the batch is a feature vector of size num_features = 1024; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
                ], 
                "version": 3
            }
        }, 
        "tf2-preview-inception_v3": {
            "family_description": "SavedModel 2.0 format; help preview TensorFlow 2.0 functionality; Inception V3 is a neural network architecture for image classification", 
            "tf2-preview-inception_v3-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed missing default <code>trainable=False</code>.</li>\n<li>Fixed broken regularization_losses.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Provides proper names for variables, fixing crash in <code>Model.save()</code>\n    (<a href=\"https://github.com/tensorflow/hub/issues/287\">GitHub issue #287</a>).</li>\n</ul>\n<h4>Version 4</h4>\n<ul>\n<li>Adds back missing update ops for batch norm that were lost in version 3,\n    (<a href=\"https://github.com/tensorflow/hub/issues/304\">GitHub issue #304</a>).</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fe62c7c51-d7bc-4611-8208-0afc7742a3fc", 
                "module_description": "feature vector has size num_features = 2048; size of the input images is fixed to height x width = 299 x 299 pixels", 
                "references": [
                    "Rethinking the Inception Architecture for Computer Vision"
                ], 
                "version": 4
            }
        }, 
        "tf2-preview-mobilenet_v2": {
            "family_description": "SavedModel 2.0 format; help preview TensorFlow 2.0 functionality; MobileNet V2 is a family of neural network architectures for efficient on-device image classification and related tasks; Mobilenets come in various sizes controlled by a multiplier for the depth (number of features) in the convolutional layers; They can also be trained for various sizes of input images to control inference speed", 
            "tf2-preview-mobilenet_v2-feature_vector": {
                "application_domain": "image; feature_vector", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed missing default <code>trainable=False</code>.</li>\n<li>Fixed broken regularization_losses.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Provides proper names for variables, fixing crash in <code>Model.save()</code>\n    (<a href=\"https://github.com/tensorflow/hub/issues/287\">GitHub issue #287</a>).</li>\n</ul>\n<h4>Version 4</h4>\n<ul>\n<li>Adds back missing update ops for batch norm that were lost in version 3,\n    (<a href=\"https://github.com/tensorflow/hub/issues/304\">GitHub issue #304</a>).</li>\n</ul>", 
                "link": "https://aihub.cloud.google.com/p/products%2Fbf481eb3-6f51-40f8-8a59-3b68c36bf54a", 
                "module_description": "TF-Slim implementation of mobilenet_v2 with a depth multiplier of 1.0 and an input size of 224x224 pixels; feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector). If you want the full model including the classification it was originally trained for, use module [google/tf2-preview/mobilenet_v2/classification/4](https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4) instead.   ## Training  The checkpoint exported into this module was mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt downloaded from [MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md). Its weights were originally obtained by training on the ILSVRC-2012-CLS dataset for image classification (\"Imagenet\").  ## Usage  This module can be used with the hub.KerasLayer as follows:  python m = tf.keras.Sequential([     hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", output_shape=[1280],                    trainable=False),  # Can be True, see below.     tf.keras.layers.Dense(num_classes, activation='softmax') ]) m.build([None, 224, 224, 3])  # Batch input shape.   The output of the module is a batch of feature vectors. For each input image, the feature vector has size num_features = 1280; size of the input images is fixed to height x width = 224 x 224 pixels", 
                "references": [
                    "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"
                ], 
                "version": 4
            }
        }
    }, 
    "image; generate": {
        "biggan": {
            "biggan-128": {
                "application_domain": "image; generate", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed race condition causing batch statistics for previous truncation value\n    to be used on the first run call for a new truncation value.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2Fe39eb6e3-a782-4384-95f7-a8fa9a42f3f0", 
                "module_description": "generate 128x128 image", 
                "references": [
                    "Large Scale GAN Training for High Fidelity Natural Image Synthesis"
                ], 
                "version": 2
            }, 
            "biggan-256": {
                "application_domain": "image; generate", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed race condition causing batch statistics for previous truncation value\n    to be used on the first run call for a new truncation value.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2Feac50b05-4c5f-43d4-ab0e-4ea51f3952c6", 
                "module_description": "generate 256x256 image", 
                "references": [
                    "Large Scale GAN Training for High Fidelity Natural Image Synthesis"
                ], 
                "version": 2
            }, 
            "biggan-512": {
                "application_domain": "image; generate", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Fixed race condition causing batch statistics for previous truncation value\n    to be used on the first run call for a new truncation value.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2F88f5be54-5705-4795-8413-2f078f1375b1", 
                "module_description": "generate 512x512 image", 
                "references": [
                    "Large Scale GAN Training for High Fidelity Natural Image Synthesis"
                ], 
                "version": 2
            }, 
            "family_description": "BigGAN image generator"
        }, 
        "biggan-deep": {
            "biggan-deep-128": {
                "application_domain": "image; generate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F6c2d2087-5133-4d3d-936b-538ef36da6ca", 
                "module_description": "generate 128x128 image", 
                "references": [
                    "Large Scale GAN Training for High Fidelity Natural Image Synthesis"
                ], 
                "version": 1
            }, 
            "biggan-deep-256": {
                "application_domain": "image; generate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fe7c0f3f6-319b-4e67-b812-7b362d4a9bcd", 
                "module_description": "generate 256x256 image", 
                "references": [
                    "Large Scale GAN Training for High Fidelity Natural Image Synthesis"
                ], 
                "version": 1
            }, 
            "biggan-deep-512": {
                "application_domain": "image; generate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F023a13aa-3380-433f-bbfd-d3508aa8ec2b", 
                "module_description": "generate 512x512 image", 
                "references": [
                    "Large Scale GAN Training for High Fidelity Natural Image Synthesis"
                ], 
                "version": 1
            }, 
            "family_description": "BigGAN-deep image generator"
        }
    }, 
    "image; generate; discriminate": {
        "compare_gan-model": {
            "compare_gan-model_10_lsun_bedroom_resnet19": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Ff04e83f1-e973-4f18-ba27-6469e5e05679", 
                "module_description": "Dataset: LSUN Bedroom; Model: Non-saturating GAN; Architecture: ResNet19; Optimizer: Adam (lr=1.281e-04, beta1=0.711, beta2=0.979); Discriminator iterations per generator iteration: 1; Discriminator normalizaton: Layer normalization; Discriminator regularization: WGAN Gradient Penalty (lambda=0.145)", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_11_cifar10_resnet_cifar": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F91ffd6ce-87b0-46e6-bbc8-44ba219a3db0", 
                "module_description": "Dataset: CIFAR-10; Model: Non-saturating GAN; Architecture: ResNet CIFAR; Optimizer: Adam (lr=2.000e-04, beta1=0.500, beta2=0.999); Discriminator iterations per generator iteration: 5; Discriminator normalizaton: none; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_12_cifar10_resnet_cifar": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F7f95f54f-7afc-4319-8b0b-5ea08a46542b", 
                "module_description": "Dataset: CIFAR-10; Model: Non-saturating GAN; Architecture: ResNet CIFAR; Optimizer: Adam (lr=1.000e-04, beta1=0.500, beta2=0.999); Discriminator iterations per generator iteration: 5; Discriminator normalizaton: none; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_13_cifar10_resnet_cifar": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F9cde4ff0-9430-4fdd-9595-e1b0e85716ff", 
                "module_description": "Dataset: CIFAR-10; Model: Non-saturating GAN; Architecture: ResNet CIFAR; Optimizer: Adam (lr=2.000e-04, beta1=0.500, beta2=0.999); Discriminator iterations per generator iteration: 5; Discriminator normalizaton: Spectral normalization; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_14_cifar10_resnet_cifar": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F5f7d0558-d96c-4351-866c-aaeaf2a7ebb0", 
                "module_description": "Dataset: CIFAR-10; Model: Non-saturating GAN; Architecture: ResNet CIFAR; Optimizer: Adam (lr=2.000e-04, beta1=0.500, beta2=0.999); Discriminator iterations per generator iteration: 5; Discriminator normalizaton: Spectral normalization; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_15_cifar10_resnet_cifar": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F450b5a4e-523c-459d-aa99-ddb678086189", 
                "module_description": "Dataset: CIFAR-10; Model: Non-saturating GAN; Architecture: ResNet CIFAR; Optimizer: Adam (lr=2.000e-04, beta1=0.500, beta2=0.999); Discriminator iterations per generator iteration: 5; Discriminator normalizaton: Spectral normalization; Discriminator regularization: WGAN Gradient Penalty (lambda=1.000)", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_1_celebahq128_resnet19": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F5cb333a1-c350-4432-b72c-227e755715b0", 
                "module_description": "Dataset: CelebA HQ; Model: Non-saturating GAN; Architecture: ResNet19; Optimizer: Adam (lr=3.381e-05, beta1=0.375, beta2=0.998); Discriminator iterations per generator iteration: 1; Discriminator normalizaton: none; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_2_celebahq128_resnet19": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F7580183a-c070-444d-82b0-b24792718d84", 
                "module_description": "Dataset: CelebA HQ; Model: Non-saturating GAN; Architecture: ResNet19; Optimizer: Adam (lr=1.000e-04, beta1=0.500, beta2=0.999); Discriminator iterations per generator iteration: 1; Discriminator normalizaton: none; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_3_lsun_bedroom_resnet19": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F5fecd953-1595-4bdc-9da4-0c5c9e765e16", 
                "module_description": "Dataset: LSUN Bedroom; Model: Least-squares GAN; Architecture: ResNet19; Optimizer: Adam (lr=3.220e-05, beta1=0.585, beta2=0.990); Discriminator iterations per generator iteration: 1; Discriminator normalizaton: none; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_4_lsun_bedroom_resnet19": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F7c9cedff-bef9-4d61-8ee3-c0653110ec9e", 
                "module_description": "Dataset: LSUN Bedroom; Model: Non-saturating GAN; Architecture: ResNet19; Optimizer: Adam (lr=1.927e-05, beta1=0.195, beta2=0.882); Discriminator iterations per generator iteration: 1; Discriminator normalizaton: none; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_5_celebahq128_resnet19": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fb07d9220-7f65-4039-b19a-60f2ea77197a", 
                "module_description": "Dataset: CelebA HQ; Model: Non-saturating GAN; Architecture: ResNet19; Optimizer: Adam (lr=1.000e-04, beta1=0.500, beta2=0.999); Discriminator iterations per generator iteration: 1; Discriminator normalizaton: Layer normalization; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_6_celebahq128_resnet19": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F7c5e0934-232d-41a3-9b4b-4603bd519934", 
                "module_description": "Dataset: CelebA HQ; Model: Non-saturating GAN; Architecture: ResNet19; Optimizer: Adam (lr=1.000e-04, beta1=0.500, beta2=0.999); Discriminator iterations per generator iteration: 1; Discriminator normalizaton: Layer normalization; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_7_lsun_bedroom_resnet19": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fc027f898-e89d-4924-b6f3-9a235ddb51fb", 
                "module_description": "Dataset: LSUN Bedroom; Model: Least-squares GAN; Architecture: ResNet19; Optimizer: Adam (lr=2.000e-04, beta1=0.500, beta2=0.999); Discriminator iterations per generator iteration: 1; Discriminator normalizaton: Spectral normalization; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_8_lsun_bedroom_resnet19": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Faa753b2b-595c-4321-b4c1-ee6d4d43418d", 
                "module_description": "Dataset: LSUN Bedroom; Model: Non-saturating GAN; Architecture: ResNet19; Optimizer: Adam (lr=2.851e-04, beta1=0.102, beta2=0.998); Discriminator iterations per generator iteration: 1; Discriminator normalizaton: Spectral normalization; Discriminator regularization: none", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "compare_gan-model_9_celebahq128_resnet19": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fbcc3bd27-2216-4d41-be27-f61d95d2437d", 
                "module_description": "Dataset: CelebA HQ; Model: Non-saturating GAN; Architecture: ResNet19; Optimizer: Adam (lr=1.000e-04, beta1=0.500, beta2=0.900); Discriminator iterations per generator iteration: 5; Discriminator normalizaton: Layer normalization; Discriminator regularization: DRAGAN Gradient Penalty (lambda=1.000)", 
                "references": [
                    "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
                ], 
                "version": 1
            }, 
            "family_description": "GAN image generator and discriminator; Use different datasets and architectures"
        }, 
        "compare_gan-s3gan": {
            "compare_gan-s3gan_20_128x128": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F7fd7121a-a84e-4d35-bb83-21e035cd16b2", 
                "module_description": "generate and discriminate 128x128 image", 
                "references": [
                    "High-Fidelity Image Generation With Fewer Labels"
                ], 
                "version": 1
            }, 
            "compare_gan-s3gan_2_5_128x128": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F65e3632b-f995-43f5-be9f-c555f21d8cc6", 
                "module_description": "generate and discriminate 128x128 image", 
                "references": [
                    "High-Fidelity Image Generation With Fewer Labels"
                ], 
                "version": 1
            }, 
            "compare_gan-s3gan_5_128x128": {
                "application_domain": "image; generate; discriminate", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F86cec259-af18-479a-bc0a-d57db391a323", 
                "module_description": "generate and discriminate 128x128 image", 
                "references": [
                    "High-Fidelity Image Generation With Fewer Labels"
                ], 
                "version": 1
            }, 
            "family_description": "S3GAN generator and discriminator."
        }
    }, 
    "text; embedding": {
        "Wiki-words": {
            "Wiki-words-250": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F12ce75a1-5f7c-4452-997c-73b9998373ad", 
                "module_description": "output shape is 250 dimensions; The module preprocesses its input by splitting on spaces", 
                "references": [
                    "Efficient Estimation of Word Representations in Vector Space"
                ], 
                "version": 1
            }, 
            "Wiki-words-250-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fda0a53b5-a301-4641-b41d-d8aec0db9753", 
                "module_description": "output shape is 250 dimensions; The module preprocesses its input by removing punctuation and splitting on spaces", 
                "references": [
                    "Efficient Estimation of Word Representations in Vector Space"
                ], 
                "version": 1
            }, 
            "Wiki-words-500": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Ffdbabad5-758b-46c7-a841-08386afa204f", 
                "module_description": "output shape is 500 dimensions; The module preprocesses its input by splitting on spaces", 
                "references": [
                    "Efficient Estimation of Word Representations in Vector Space"
                ], 
                "version": 1
            }, 
            "Wiki-words-500-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fae958f02-81f5-43ce-acfe-469b1fb18517", 
                "module_description": "output shape is 500 dimensions; The module preprocesses its input by removing punctuation and splitting on spaces", 
                "references": [
                    "Efficient Estimation of Word Representations in Vector Space"
                ], 
                "version": 1
            }, 
            "family_description": "Text embedding based on skipgram version of word2vec with 1 out-of-vocabulary bucket. Maps from text to x-dimensional embedding vectors"
        }, 
        "nnlm": {
            "family_description": "Text embedding based on feed-forward Neural-Net Language Models with pre-built OOV", 
            "nnlm-de-dim128": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F581bb5fd-cc8f-4adf-bc47-6b2fba56ed82", 
                "module_description": "Language: de; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-de-dim128-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F0d2e953e-7f38-4265-9e93-2095b2dd1c5c", 
                "module_description": "Language: de; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-de-dim50": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F79ccd5a6-b604-448f-a71c-ea818a7ebc44", 
                "module_description": "Language: de; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-de-dim50-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F4bc8df1d-c51e-4727-8c00-c728f8bb28ee", 
                "module_description": "Language: de; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-en-dim128": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fa3184819-f94d-402c-9b0e-854b10d03db9", 
                "module_description": "Language: en; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-en-dim128-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fd62c150f-ba94-4a0e-af48-028f0740c766", 
                "module_description": "Language: en; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-en-dim50": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Feb4cd7ec-6549-4b4d-bb00-eaa1de9fa3cb", 
                "module_description": "Language: en; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-en-dim50-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fd0b6d1fb-7152-43b6-b8bc-f1776be3211b", 
                "module_description": "Language: en; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-es-dim128": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Ff6a82be2-a31e-4630-b022-d746518a83ba", 
                "module_description": "Language: es; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-es-dim128-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fad884d6a-a003-4a85-8653-76bdaff093f2", 
                "module_description": "Language: es; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-es-dim50": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Ff7bf57ae-39e2-415e-b6c4-7066e03d0fd4", 
                "module_description": "Language: es; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-es-dim50-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F21cbb434-e8f4-4155-8084-7530b7b17fea", 
                "module_description": "Language: es; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-id-dim128": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F3e58488d-9270-4884-ba92-aecb57373ec1", 
                "module_description": "Language: id; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-id-dim128-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F4b795742-90ab-4942-8907-dbe98884e9db", 
                "module_description": "Language: id; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-id-dim50": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F09702e46-753b-4dad-80bf-d0dda1b0421f", 
                "module_description": "Language: id; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-id-dim50-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F0a8e3c49-c4f4-4c85-a8ff-307b6e6708bf", 
                "module_description": "Language: id; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-ja-dim128": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fd22a0516-404f-4737-83fa-eefaaa6952e5", 
                "module_description": "Language: ja; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-ja-dim128-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F66533fdb-a87d-4415-ad5a-d72aab38269f", 
                "module_description": "Language: ja; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-ja-dim50": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F187a1ce7-a131-4eda-b26f-e01eee47246e", 
                "module_description": "Language: ja; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-ja-dim50-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F75f7408b-31e7-49e2-958b-50e380e2b21a", 
                "module_description": "Language: ja; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-ko-dim128": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Ffd04f17e-9939-41c0-a2b7-7789878f7b7d", 
                "module_description": "Language: ko; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-ko-dim128-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Ff405f842-d604-4b1c-b074-0d16a0ff225d", 
                "module_description": "Language: ko; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-ko-dim50": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F33345f54-4152-4228-933a-03ffdc729c02", 
                "module_description": "Language: ko; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-ko-dim50-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F8dd3f7e0-98b7-4d1d-aff8-fdf98e8d0528", 
                "module_description": "Language: ko; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-zh-dim128": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F5adc3e79-881d-4450-95ad-6a9af0df7f22", 
                "module_description": "Language: zh; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-zh-dim128-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Faf41efc4-c8aa-4b0c-a13c-39abcd2ddfd4", 
                "module_description": "Language: zh; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-zh-dim50": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F3f91c2fb-6eb8-4539-96cf-2ab91ac39e3a", 
                "module_description": "Language: zh; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "nnlm-zh-dim50-with-normalization": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F4c36797c-3d02-4332-8c8f-16f1d5c23347", 
                "module_description": "Language: zh; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by removing punctuation and splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }
        }, 
        "random-nnlm": {
            "family_description": "Text embedding initialized with some random normal tensor; It contains no \"knowledge\", but can conveniently be used as a baseline when comparing to other modules; Vocabulary of the module is based on other nnlm modules in TFHub", 
            "random-nnlm-en-dim128": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fa52e9466-f691-4f91-a62e-f6e39eb967f8", 
                "module_description": "Text embedding initialized with tf.random_normal([vocabulary_size, 128]); Vocabulary of the module is based on nnlm-en-dim128", 
                "references": null, 
                "version": 1
            }, 
            "random-nnlm-en-dim50": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fb0766fd5-82b5-48e3-9d6f-26c5d2601d5f", 
                "module_description": "Text embedding initialized with tf.random_normal([vocabulary_size, 50]); Vocabulary of the module is based on nnlm-en-dim50", 
                "references": null, 
                "version": 1
            }
        }, 
        "tf2-preview-gnews-swivel": {
            "family_description": "SavedModel 2.0 format; help preview TensorFlow 2.0 functionality; Text embedding based on Swivel co-occurrence matrix factorization with pre-built OOV", 
            "tf2-preview-gnews-swivel-20dim": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F6b1c1b14-d974-44d8-a131-4c132be4b26d", 
                "module_description": "Map from text to 20-dimensional vectors; Created using Swivel matrix factorization method; Input preprocessed by splitting on spaces", 
                "references": [
                    "Swivel: Improving Embeddings by Noticing What's Missing"
                ], 
                "version": 1
            }, 
            "tf2-preview-gnews-swivel-20dim-with-oov": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F18fdd467-ce81-48d8-aa41-058bccff5432", 
                "module_description": "Map from text to 20-dimensional vectors; Created using Swivel matrix factorization method; Input preprocessed by splitting on spaces", 
                "references": [
                    "Swivel: Improving Embeddings by Noticing What's Missing"
                ], 
                "version": 1
            }
        }, 
        "tf2-preview-nnlm": {
            "family_description": "SavedModel 2.0 format; help preview TensorFlow 2.0 functionality; Text embedding based on feed-forward Neural-Net Language Models with pre-built OOV", 
            "tf2-preview-nnlm-en-dim128": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F89613833-f812-4e13-adb9-7e1078904f3e", 
                "module_description": "Language: en; Map from text to 128-dimensional vectors; Based on NNLM with three hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }, 
            "tf2-preview-nnlm-en-dim50": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Ff0382a1c-6527-4e76-9231-267ad3d34206", 
                "module_description": "Language: en; Map from text to 50-dimensional vectors; Based on NNLM with two hidden layers; Input preprocessed by splitting on spaces", 
                "references": [
                    "A Neural Probabilistic Language Model"
                ], 
                "version": 1
            }
        }, 
        "universal-sentence-encoder": {
            "family_description": "The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks", 
            "universal-sentence-encoder": {
                "application_domain": "text; embedding", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Exposed internal variables as Trainable.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2F79c78579-2208-4f1c-ab08-c0d94f2adad1", 
                "module_description": "This module is about 1GB; output is a 512 dimensional vector", 
                "references": [
                    "Universal Sentence Encoder"
                ], 
                "version": 2
            }, 
            "universal-sentence-encoder-large": {
                "application_domain": "text; embedding", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Exposed internal variables as Trainable.</li>\n</ul>\n<h4>Version 3</h4>\n<ul>\n<li>Fixed batch invariant <a href=\"https://github.com/tensorflow/hub/issues/74\">bug</a>. This\n   version was retrained and its embedding space differs from previous versions.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2F42c1bfd4-8104-450c-a348-29b047d3691c", 
                "module_description": "This module is about 800MB; output is a 512 dimensional vector", 
                "references": [
                    "Universal Sentence Encoder"
                ], 
                "version": 3
            }, 
            "universal-sentence-encoder-lite": {
                "application_domain": "text; embedding", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n<h4>Version 2</h4>\n<ul>\n<li>Exposed internal variables as Trainable.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2Ffca281b8-15fd-4a79-824b-8f3228b40fa7", 
                "module_description": "lightweight version; output is a 512 dimensional vector", 
                "references": [
                    "Universal Sentence Encoder"
                ], 
                "version": 2
            }
        }, 
        "universal-sentence-encoder-xling": {
            "family_description": "The Universal Sentence Encoder Cross-lingual (XLING) module is an extension of the Universal Sentence Encoder that includes training on multiple tasks across languages", 
            "universal-sentence-encoder-xling-en-de": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F5aa67e0e-389a-4ad7-bac6-fe1f705bc689", 
                "module_description": "English and German (en-de); output is a 512 dimensional vector", 
                "references": [
                    "Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model.", 
                    "Universal Sentence Encoder"
                ], 
                "version": 1
            }, 
            "universal-sentence-encoder-xling-en-es": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F774a1c7d-4426-41da-a964-b152fbe89c54", 
                "module_description": "English and Spanish (en-es); output is a 512 dimensional vector", 
                "references": [
                    "Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model.", 
                    "Universal Sentence Encoder"
                ], 
                "version": 1
            }, 
            "universal-sentence-encoder-xling-en-fr": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fd7ba5172-43d6-4860-84b8-daa5165e766b", 
                "module_description": "English and French (en-fr); output is a 512 dimensional vector", 
                "references": [
                    "Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model.", 
                    "Universal Sentence Encoder"
                ], 
                "version": 1
            }, 
            "universal-sentence-encoder-xling-many": {
                "application_domain": "text; embedding", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2F1c473955-f951-42cc-9191-df4e6dce0ceb", 
                "module_description": "English, French, German, Spanish, Italian, Chinese, Korean, and Japanese; output is a 512 dimensional vector", 
                "references": [
                    "Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model.", 
                    "Universal Sentence Encoder"
                ], 
                "version": 1
            }
        }
    }, 
    "text; representation": {
        "bert": {
            "bert_cased_L-12_H-768_A-12": {
                "application_domain": "text; representation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2F2c1fe4d8-4ff3-4d4f-8ac4-45d445532a3b", 
                "module_description": "cased input; 12-layer; 768-hidden; 12-heads", 
                "references": [
                    "BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding"
                ], 
                "version": 1
            }, 
            "bert_cased_L-24_H-1024_A-16": {
                "application_domain": "text; representation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2Fadd1e4fb-a853-4a8e-96e4-e2a5b470438f", 
                "module_description": "cased input; 24-layer; 1024-hidden; 16-heads", 
                "references": [
                    "BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding"
                ], 
                "version": 1
            }, 
            "bert_chinese_L-12_H-768_A-12": {
                "application_domain": "text; representation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2Ff28d5f8a-0fa9-4f79-b70f-da39924c3c9b", 
                "module_description": "chinese input; 12-layer; 768-hidden; 12-heads", 
                "references": [
                    "BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding"
                ], 
                "version": 1
            }, 
            "bert_multi_cased_L-12_H-768_A-12": {
                "application_domain": "text; representation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2F9673daa5-5abe-41e6-9392-7feae1ce56fc", 
                "module_description": "multi-cased input; 12-layer; 768-hidden; 12-heads", 
                "references": [
                    "BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding"
                ], 
                "version": 1
            }, 
            "bert_uncased_L-12_H-768_A-12": {
                "application_domain": "text; representation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2Fac1806a5-a32c-4adf-b069-074156efc5c3", 
                "module_description": "uncased input; 12-layer; 768-hidden; 12-heads", 
                "references": [
                    "BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding"
                ], 
                "version": 1
            }, 
            "bert_uncased_L-24_H-1024_A-16": {
                "application_domain": "text; representation", 
                "changelog": "<h4>Version 1</h4>\n<ul>\n<li>Initial release.</li>\n</ul>\n", 
                "link": "https://aihub.cloud.google.com/p/products%2F46b652ee-80d4-4bcd-b707-1e4dcdd88759", 
                "module_description": "uncased input; 24-layer; 1024-hidden; 16-heads", 
                "references": [
                    "BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding"
                ], 
                "version": 1
            }, 
            "family_description": "This modules outputs a representations for every token in the input sequence and a pooled representation of the entire input"
        }
    }, 
    "video; classification": {
        "i3d": {
            "family_description": "A video classification model; This model achieved state-of-the-art results on the UCF101 and HMDB51 datasets from fine-tuning these models; I3D models pre-trained on Kinetics also placed first in the CVPR 2017", 
            "i3d-kinetics-400": {
                "application_domain": "video; classification", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Ff17b9324-6c9f-4e0b-8601-2919881444c8", 
                "module_description": "It can label 400 different actions", 
                "references": [
                    "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"
                ], 
                "version": 1
            }, 
            "i3d-kinetics-600": {
                "application_domain": "video; classification", 
                "changelog": null, 
                "link": "https://aihub.cloud.google.com/p/products%2Fd74ea7fa-91a3-4501-9262-1c18fe957bb7", 
                "module_description": "It can label 600 different actions", 
                "references": [
                    "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"
                ], 
                "version": 1
            }
        }
    }
}