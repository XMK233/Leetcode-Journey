## 1. bert

**This modules outputs a representations for every token in the input sequence and a pooled representation of the entire input**

### 1.1 [bert\_cased\_L-12\_H-768\_A-12@1](https://aihub.cloud.google.com/p/products%2F2c1fe4d8-4ff3-4d4f-8ac4-45d445532a3b)


Module Description: 

* cased input

*  12-layer

*  768-hidden

*  12-heads


References: 

* *BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding*


Changelog: 

    <h4>Version 1</h4>

    <ul>

    <li>Initial release.</li>

    </ul>

    

### 1.2 [bert\_cased\_L-24\_H-1024\_A-16@1](https://aihub.cloud.google.com/p/products%2Fadd1e4fb-a853-4a8e-96e4-e2a5b470438f)


Module Description: 

* cased input

*  24-layer

*  1024-hidden

*  16-heads


References: 

* *BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding*


Changelog: 

    <h4>Version 1</h4>

    <ul>

    <li>Initial release.</li>

    </ul>

    

### 1.3 [bert\_chinese\_L-12\_H-768\_A-12@1](https://aihub.cloud.google.com/p/products%2Ff28d5f8a-0fa9-4f79-b70f-da39924c3c9b)


Module Description: 

* chinese input

*  12-layer

*  768-hidden

*  12-heads


References: 

* *BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding*


Changelog: 

    <h4>Version 1</h4>

    <ul>

    <li>Initial release.</li>

    </ul>

    

### 1.4 [bert\_multi\_cased\_L-12\_H-768\_A-12@1](https://aihub.cloud.google.com/p/products%2F9673daa5-5abe-41e6-9392-7feae1ce56fc)


Module Description: 

* multi-cased input

*  12-layer

*  768-hidden

*  12-heads


References: 

* *BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding*


Changelog: 

    <h4>Version 1</h4>

    <ul>

    <li>Initial release.</li>

    </ul>

    

### 1.5 [bert\_uncased\_L-12\_H-768\_A-12@1](https://aihub.cloud.google.com/p/products%2Fac1806a5-a32c-4adf-b069-074156efc5c3)


Module Description: 

* uncased input

*  12-layer

*  768-hidden

*  12-heads


References: 

* *BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding*


Changelog: 

    <h4>Version 1</h4>

    <ul>

    <li>Initial release.</li>

    </ul>

    

### 1.6 [bert\_uncased\_L-24\_H-1024\_A-16@1](https://aihub.cloud.google.com/p/products%2F46b652ee-80d4-4bcd-b707-1e4dcdd88759)


Module Description: 

* uncased input

*  24-layer

*  1024-hidden

*  16-heads


References: 

* *BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding*


Changelog: 

    <h4>Version 1</h4>

    <ul>

    <li>Initial release.</li>

    </ul>

    

