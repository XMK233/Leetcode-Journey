{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## First let's get the .md code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, json, md2py\n",
    "import os, shutil, re, json, tqdm, string\n",
    "from lxml import etree\n",
    "\n",
    "JSON_FILE = r\"J:\\ModelStoreData\\AIHub\\2019-06-04\\TFModule_Info.json\"\n",
    "\n",
    "def get_element_by_xpath(page_source, xpath, index = 0):\n",
    "    res = None\n",
    "    try:\n",
    "        _ = page_source.xpath(xpath)[index]\n",
    "        ## eliminate unnecessary chars.\n",
    "        _ = _.replace(\"\\n\", \" \").strip()\n",
    "        ## change the strings to lower case. Hopefully it will make the comparison standardized.\n",
    "        ## be careful about the following change. If it is not suitable to the situation, stop it without hesitation.\n",
    "#         _ = _.lower()\n",
    "        res = ' '.join(_.split())\n",
    "    except:\n",
    "        res = None\n",
    "    finally:\n",
    "        return res\n",
    "\n",
    "def parse_html_specially_for_imagenet(html, *args):\n",
    "    res = None\n",
    "    try:\n",
    "        page_source = etree.HTML(html)\n",
    "        author = get_element_by_xpath(page_source, \"//ul/li/text()\", args[0])\n",
    "        author = author.replace(\":\", \"\")\n",
    "        paper = get_element_by_xpath(page_source, \"//ul/li/a/text()\", args[1])\n",
    "        paper = paper.replace(\"\\\"\", \"\")\n",
    "        link_to_paper = get_element_by_xpath(page_source, \"//ul/li/a/@href\", args[2])\n",
    "        year = get_element_by_xpath(page_source, \"//ul/li/text()\", args[3])\n",
    "        year = re.findall(r\"(\\d{4})\", year)[0]\n",
    "        res = {\"author\": author, \"paper\": paper, \"link_to_paper\":link_to_paper, \"year\": year}\n",
    "    except:\n",
    "        print(f\"failed\")\n",
    "    finally:\n",
    "        return res\n",
    "    \n",
    "def eliminate_punctuation_and_change_to_lower_case(_):\n",
    "    exclude = set(string.punctuation)\n",
    "    paper_name = _.lower()\n",
    "    paper_name = ''.join(ch for ch in paper_name if ch not in exclude).strip()\n",
    "    return paper_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Overview\n",
      "\n",
      "MobileNet V1 is a family of neural network architectures for efficient\n",
      "on-device image classification, originally published by\n",
      "\n",
      "  * Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,\n",
      "    Tobias Weyand, Marco Andreetto, Hartwig Adam:\n",
      "    [\"MobileNets: Efficient Convolutional Neural Networks for\n",
      "    Mobile Vision Applications\"](https://arxiv.org/abs/1704.04861), 2017.\n",
      "\n",
      "Mobilenets come in various sizes controlled by a multiplier for the\n",
      "depth (number of features) in the convolutional layers. They can also be\n",
      "trained for various sizes of input images to control inference speed.\n",
      "This TF-Hub module uses the TF-Slim implementation of\n",
      "`mobilenet_v1_v1_075`, **instrumented for quantization**,\n",
      "with a depth multiplier of 0.75 and an input size of\n",
      "192x192 pixels.\n",
      "\n",
      "The module contains a trained instance of the network, packaged to get\n",
      "[feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector).\n",
      "If you want the full model including the classification it was originally\n",
      "trained for, use module\n",
      "[`google/imagenet/mobilenet_v1_075_192/quantops/classification/1`](https://tfhub.dev/google/imagenet/mobilenet_v1_075_192/quantops/classification/1)\n",
      "instead.\n",
      "\n",
      "\n",
      "## Quantization\n",
      "\n",
      "This module is meant for use in models whose weights will be quantized to\n",
      "`uint8` by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/)\n",
      "for deployment to mobile devices.\n",
      "\n",
      "The trained weights of this module are shipped as `float32` numbers,\n",
      "but its graph has been augmented by `tf.contrib.quantize` with extra ops\n",
      "that simulate the effect of quantization already during training,\n",
      "so that the model can adjust to it.\n",
      "\n",
      "## Training\n",
      "\n",
      "The checkpoint exported into this module was `mobilenet_v1_2018_02_22/mobilenet_v1_0.75_192_quant/mobilenet_v1_0.75_192_quant.ckpt` downloaded\n",
      "from\n",
      "[MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md).\n",
      "Its weights were originally obtained by training on the ILSVRC-2012-CLS\n",
      "dataset for image classification (\"Imagenet\"), with simulated quantization.\n",
      "\n",
      "## Usage\n",
      "\n",
      "This module implements the common signature for computing\n",
      "[image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector).\n",
      "It can be used like\n",
      "\n",
      "```python\n",
      "module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_075_192/quantops/feature_vector/1\")\n",
      "height, width = hub.get_expected_image_size(module)\n",
      "images = ...  # A batch of images with shape [batch_size, height, width, 3].\n",
      "features = module(images)  # Features with shape [batch_size, num_features].\n",
      "```\n",
      "\n",
      "...or using the signature name `image_feature_vector`. The output for each image\n",
      "in the batch is a feature vector of size `num_features` = 768.\n",
      "\n",
      "For this module, the size of the input image is fixed to\n",
      "`height` x `width` = 192 x 192 pixels.\n",
      "The input `images` are expected to have color values in the range [0,1],\n",
      "following the\n",
      "[common image input](https://www.tensorflow.org/hub/common_signatures/images#input)\n",
      "conventions.\n",
      "\n",
      "\n",
      "## Fine-tuning\n",
      "\n",
      "The current version of this module only provides an inference graph\n",
      "and cannot be fine-tuned.\n"
     ]
    }
   ],
   "source": [
    "set_imagenet = []\n",
    "set_eference = []\n",
    "set_tf2_preview = []\n",
    "set_inaturalist_inception_v3 = []\n",
    "set_image_augmentation = []\n",
    "set_others = []\n",
    "\n",
    "model_paper = {}\n",
    "\n",
    "model_url = {}\n",
    "\n",
    "paper_url = {}\n",
    "\n",
    "with open(JSON_FILE, \"r\") as jf:\n",
    "    items = json.load(jf)\n",
    "\n",
    "for item in items:\n",
    "    name = item[\"name\"]\n",
    "    version = item[\"version\"]\n",
    "    info = item[\"info\"]\n",
    "    link_url = \"https://aihub.cloud.google.com/p/products%2F{}\".format(info[1][1].split(\"/\")[1])\n",
    "    md = info[1][7]\n",
    "    \n",
    "    print(md)\n",
    "    break\n",
    "    \n",
    "    ## store the url\n",
    "    model_url[name] = link_url\n",
    "    \n",
    "    ## lets' first focus on the models that contains \"imagenet\".\n",
    "    ## around 150 modules used imagenet. about 315 different versions.\n",
    "    ## the websites of them are pretty alike.\n",
    "    if \"imagenet\" in name.split(\"-\")[0]:\n",
    "        set_imagenet.append(item)\n",
    "        toc = md2py.md2py(md)\n",
    "        ## all the info will be in \"reference\".\n",
    "        reference = parse_html_specially_for_imagenet(str(toc.source), 0, 0, 0, 1)\n",
    "        ##\n",
    "        if reference != None:\n",
    "            paper = reference[\"paper\"]\n",
    "            paper_url[paper] = reference[\"link_to_paper\"]\n",
    "            model_paper[name] = reference\n",
    "        else:\n",
    "            print(\"failed: {} {} {}\".format(name, version, link_url))\n",
    "        \n",
    "    \n",
    "    ## these models have specified reference part.\n",
    "    ## generally speaking, the reference part is the last part of the page.\n",
    "    elif \"eference\" in md:\n",
    "        set_eference.append(item)\n",
    "        toc = md2py.md2py(md)\n",
    "        page_source = etree.HTML(str(toc.source))\n",
    "        ## paper name could be in the following two xpathes.\n",
    "        combi1 = page_source.xpath(\"//p[last()]/text()\")\n",
    "        combi2 = page_source.xpath(\"//a[last()]/text()\")\n",
    "        ## the best part is in \"paper\"\n",
    "        if \"universal-sentence-encoder-xling\" in name:\n",
    "            paper = \"Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model.\" ## combi1 will be: ['[1] M. Chidambaram, Y. Yang, D. Cer, S. Yuan, Y.-H. Sung, B. Strope, and R.\\nKurzweil. Learning Cross-Lingual Sentence Representations via a Multi-task\\nDual-Encoder Model. ArXiv e-prints, October 2018.']\n",
    "        else:\n",
    "            paper = combi2[-1].replace(\"\\n\", \"\")\n",
    "        ##        \n",
    "#         paper = eliminate_punctuation_and_change_to_lower_case(paper)\n",
    "        if paper not in model_paper:\n",
    "                model_paper[name] = paper\n",
    "    else:\n",
    "        if \"tf2-preview\" in name:\n",
    "            set_tf2_preview.append(item)\n",
    "            toc = md2py.md2py(md)\n",
    "            ## all the info will be in \"reference\".\n",
    "            reference = parse_html_specially_for_imagenet(str(toc.source), 0, 0, 0, 1)\n",
    "            ##\n",
    "            if reference != None:\n",
    "                paper = reference[\"paper\"]\n",
    "                paper_url[paper] = reference[\"link_to_paper\"]\n",
    "                model_paper[name] = reference\n",
    "            else:\n",
    "                print(\"failed: {} {} {}\".format(name, version, link_url))\n",
    "        else:\n",
    "            if \"inaturalist-inception_v3\" in name:\n",
    "                set_inaturalist_inception_v3.append(item)\n",
    "                toc = md2py.md2py(md)\n",
    "                ## all the info will be in \"reference\".\n",
    "                reference = parse_html_specially_for_imagenet(str(toc.source), 0, 0, 0, 1)\n",
    "                ##\n",
    "                if reference != None:\n",
    "                    paper = reference[\"paper\"]\n",
    "                    paper_url[paper] = reference[\"link_to_paper\"]\n",
    "                    model_paper[name] = reference\n",
    "                else:\n",
    "                    print(\"failed: {} {} {}\".format(name, version, link_url))\n",
    "                    \n",
    "            elif \"image_augmentation\" in name:\n",
    "                set_image_augmentation.append(item)\n",
    "            else:\n",
    "                set_others.append(item)\n",
    "                print(name, version, link_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markdown2\n",
      "  Downloading https://files.pythonhosted.org/packages/89/d2/6aef45472f7956646fdc4cc5284e6f9d00476497a8d167c30b0f78bb75a5/markdown2-2.3.8-py2.py3-none-any.whl\n",
      "Installing collected packages: markdown2\n",
      "Successfully installed markdown2-2.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install markdown2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown2\n",
    "markdown2.markdown(\"\"\"## Overview\n",
    "\n",
    "MobileNet V1 is a family of neural network architectures for efficient\n",
    "on-device image classification, originally published by\n",
    "\n",
    "  * Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,\n",
    "    Tobias Weyand, Marco Andreetto, Hartwig Adam:\n",
    "    [\"MobileNets: Efficient Convolutional Neural Networks for\n",
    "    Mobile Vision Applications\"](https://arxiv.org/abs/1704.04861), 2017.\n",
    "\n",
    "Mobilenets come in various sizes controlled by a multiplier for the\n",
    "depth (number of features) in the convolutional layers. They can also be\n",
    "trained for various sizes of input images to control inference speed.\n",
    "This TF-Hub module uses the TF-Slim implementation of\n",
    "`mobilenet_v1_v1_075`, **instrumented for quantization**,\n",
    "with a depth multiplier of 0.75 and an input size of\n",
    "192x192 pixels.\n",
    "\n",
    "The module contains a trained instance of the network, packaged to get\n",
    "[feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector).\n",
    "If you want the full model including the classification it was originally\n",
    "trained for, use module\n",
    "[`google/imagenet/mobilenet_v1_075_192/quantops/classification/1`](https://tfhub.dev/google/imagenet/mobilenet_v1_075_192/quantops/classification/1)\n",
    "instead.\n",
    "\n",
    "\n",
    "## Quantization\n",
    "\n",
    "This module is meant for use in models whose weights will be quantized to\n",
    "`uint8` by [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/)\n",
    "for deployment to mobile devices.\n",
    "\n",
    "The trained weights of this module are shipped as `float32` numbers,\n",
    "but its graph has been augmented by `tf.contrib.quantize` with extra ops\n",
    "that simulate the effect of quantization already during training,\n",
    "so that the model can adjust to it.\n",
    "\n",
    "## Training\n",
    "\n",
    "The checkpoint exported into this module was `mobilenet_v1_2018_02_22/mobilenet_v1_0.75_192_quant/mobilenet_v1_0.75_192_quant.ckpt` downloaded\n",
    "from\n",
    "[MobileNet pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md).\n",
    "Its weights were originally obtained by training on the ILSVRC-2012-CLS\n",
    "dataset for image classification (\"Imagenet\"), with simulated quantization.\n",
    "\n",
    "## Usage\n",
    "\n",
    "This module implements the common signature for computing\n",
    "[image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector).\n",
    "It can be used like\n",
    "\n",
    "```python\n",
    "module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v1_075_192/quantops/feature_vector/1\")\n",
    "height, width = hub.get_expected_image_size(module)\n",
    "images = ...  # A batch of images with shape [batch_size, height, width, 3].\n",
    "features = module(images)  # Features with shape [batch_size, num_features].\n",
    "```\n",
    "\n",
    "...or using the signature name `image_feature_vector`. The output for each image\n",
    "in the batch is a feature vector of size `num_features` = 768.\n",
    "\n",
    "For this module, the size of the input image is fixed to\n",
    "`height` x `width` = 192 x 192 pixels.\n",
    "The input `images` are expected to have color values in the range [0,1],\n",
    "following the\n",
    "[common image input](https://www.tensorflow.org/hub/common_signatures/images#input)\n",
    "conventions.\n",
    "\n",
    "\n",
    "## Fine-tuning\n",
    "\n",
    "The current version of this module only provides an inference graph\n",
    "and cannot be fine-tuned.\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set_imagenet = []\n",
    "set_eference = []\n",
    "set_tf2_preview = []\n",
    "set_inaturalist_inception_v3 = []\n",
    "set_image_augmentation = []\n",
    "set_others = []\n",
    "\n",
    "model_paper = {}\n",
    "\n",
    "model_url = {}\n",
    "\n",
    "paper_url = {}\n",
    "\n",
    "with open(JSON_FILE, \"r\") as jf:\n",
    "    items = json.load(jf)\n",
    "\n",
    "for item in items:\n",
    "    name = item[\"name\"]\n",
    "    version = item[\"version\"]\n",
    "    info = item[\"info\"]\n",
    "    link_url = \"https://aihub.cloud.google.com/p/products%2F{}\".format(info[1][1].split(\"/\")[1])\n",
    "    md = info[1][7]\n",
    "    \n",
    "    ## store the url\n",
    "    model_url[name] = link_url\n",
    "    \n",
    "    ## lets' first focus on the models that contains \"imagenet\".\n",
    "    ## around 150 modules used imagenet. about 315 different versions.\n",
    "    ## the websites of them are pretty alike.\n",
    "    if \"imagenet\" in name.split(\"-\")[0]:\n",
    "        set_imagenet.append(item)\n",
    "        toc = md2py.md2py(md)\n",
    "        ## all the info will be in \"reference\".\n",
    "        reference = parse_html_specially_for_imagenet(str(toc.source), 0, 0, 0, 1)\n",
    "        ##\n",
    "        if reference != None:\n",
    "            paper = reference[\"paper\"]\n",
    "            paper_url[paper] = reference[\"link_to_paper\"]\n",
    "#             paper = eliminate_punctuation_and_change_to_lower_case(paper)\n",
    "            if paper not in model_paper:\n",
    "                model_paper[name] = reference #paper\n",
    "        else:\n",
    "            print(\"failed: {} {} {}\".format(name, version, link_url))\n",
    "        \n",
    "    \n",
    "    ## these models have specified reference part.\n",
    "    ## generally speaking, the reference part is the last part of the page.\n",
    "    elif \"eference\" in md:\n",
    "        set_eference.append(item)\n",
    "        toc = md2py.md2py(md)\n",
    "        page_source = etree.HTML(str(toc.source))\n",
    "        ## paper name could be in the following two xpathes.\n",
    "        combi1 = page_source.xpath(\"//p[last()]/text()\")\n",
    "        combi2 = page_source.xpath(\"//a[last()]/text()\")\n",
    "        ## the best part is in \"paper\"\n",
    "        if \"universal-sentence-encoder-xling\" in name:\n",
    "            paper = \"Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model.\" ## combi1 will be: ['[1] M. Chidambaram, Y. Yang, D. Cer, S. Yuan, Y.-H. Sung, B. Strope, and R.\\nKurzweil. Learning Cross-Lingual Sentence Representations via a Multi-task\\nDual-Encoder Model. ArXiv e-prints, October 2018.']\n",
    "        else:\n",
    "            paper = combi2[-1].replace(\"\\n\", \"\")\n",
    "        ##        \n",
    "#         paper = eliminate_punctuation_and_change_to_lower_case(paper)\n",
    "        if paper not in model_paper:\n",
    "                model_paper[name] = paper\n",
    "    else:\n",
    "        if \"tf2-preview\" in name:\n",
    "            set_tf2_preview.append(item)\n",
    "            toc = md2py.md2py(md)\n",
    "            ## all the info will be in \"reference\".\n",
    "            reference = parse_html_specially_for_imagenet(str(toc.source), 0, 0, 0, 1)\n",
    "            ##\n",
    "            if reference != None:\n",
    "                paper = reference[\"paper\"]\n",
    "#                 paper = eliminate_punctuation_and_change_to_lower_case(paper)\n",
    "                if paper not in model_paper:\n",
    "                    model_paper[name] = reference #paper\n",
    "            else:\n",
    "                print(\"failed: {} {} {}\".format(name, version, link_url))\n",
    "        else:\n",
    "            if \"inaturalist-inception_v3\" in name:\n",
    "                set_inaturalist_inception_v3.append(item)\n",
    "                toc = md2py.md2py(md)\n",
    "                ## all the info will be in \"reference\".\n",
    "                reference = parse_html_specially_for_imagenet(str(toc.source), 0, 0, 0, 1)\n",
    "                ##\n",
    "                if reference != None:\n",
    "                    paper = reference[\"paper\"]\n",
    "#                     paper = eliminate_punctuation_and_change_to_lower_case(paper)\n",
    "                    if paper not in model_paper:\n",
    "                        model_paper[name] = reference #paper\n",
    "                else:\n",
    "                    print(\"failed: {} {} {}\".format(name, version, link_url))\n",
    "                    \n",
    "            elif \"image_augmentation\" in name:\n",
    "                set_image_augmentation.append(item)\n",
    "            else:\n",
    "                set_others.append(item)\n",
    "                print(name, version, link_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Through the running of previous code, we can get following lists. \n",
    "* Key: model name\n",
    "* Value: paper name. All of the paper name is in lower case and no punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularized evolution for image classifier architecture search\n",
      "learning transferable architectures for scalable image recognition\n",
      "autoaugment learning augmentation policies from data\n"
     ]
    }
   ],
   "source": [
    "print(eliminate_punctuation_and_change_to_lower_case(\"Regularized Evolution for Image Classifier Architecture Search\"))\n",
    "print(eliminate_punctuation_and_change_to_lower_case(\"Learning Transferable Architectures for Scalable Image Recognition\"))\n",
    "print(eliminate_punctuation_and_change_to_lower_case(\"AutoAugment: Learning Augmentation Policies from Data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this markdown cell, anything says \".... have references (actual paper name)\"/ \"... referred to (actual paper name)\"/ \".... (referred models)\", means that such references are not added in the previous *model_paper*\n",
    "\n",
    "### set_imagenet\n",
    "A lot of models' name start with \"imagenet\": 150+ modules/ ~316 versions. The website layouts are the same. \n",
    "\n",
    "Noted that: __imagenet-amoebanet_a_n18_f448-classification__ and __imagenet-amoebanet_a_n18_f448-feature_vector__ have 2 references: __*regularized evolution for image classifier architecture search*__ & __*learning transferable architectures for scalable image recognition*__. \n",
    "\n",
    "\n",
    "### set_eference\n",
    "Why __eference__ rather than __reference__? Just in case some models use __Reference__, some use __reference__.  \n",
    "\n",
    "Via observation, we find that the models whose names have __universal-sentence-encoder-xling__ have special reference format that need not to be extracted by xpath or normal expression. So we just hard code them in code.  \n",
    "\n",
    "### set_tf2_preview\n",
    "A series of models that have similar webpage structure with __set_imagenet__.\n",
    "\n",
    "### set_inaturalist_inception_v3\n",
    "A series of models that have similar webpage structure with __set_imagenet__.\n",
    "\n",
    "### set_image_augmentation\n",
    "6 models that have __image_augmentation__ in their names. \n",
    "\n",
    "Among them, __image_augmentation/nas_svhn__, __image_augmentation/nas_imagenet__ and __image_augmentation/nas_cifar__ say they referred to __*autoaugment learning augmentation policies from data*__. The developer of the TFModule also says: *This module reuses implementation of an image augmentation policy from authors of the paper.*\n",
    "\n",
    "__image_augmentation/crop_rotate_color__, __image_augmentation/crop_color__ and __image_augmentation/flipx_crop_rotate_color__ don't clarify their references. Though their webpages look like the previous three, but I don't regard so for certain. \n",
    "\n",
    "### set_others \n",
    "In these modules, no reference part is specified. \n",
    "\n",
    "#### 2 models clarify their referred stuff: \n",
    "faster_rcnn-openimages_v4-inception_resnet_v2 1 https://aihub.cloud.google.com/p/products%2F41b42dfa-e600-4a73-a425-7c5c4d511c3c\n",
    "* Only has some referred models and datasets: *FasterRCNN+InceptionResNetV2 network trained on Open Images V4*\n",
    "\n",
    "openimages_v4-ssd-mobilenet_v2 1 https://aihub.cloud.google.com/p/products%2F8c6878ba-d32d-411d-bac2-2f884b748c4f\n",
    "* Only has some referred models and datasets: *SSD+MobileNetV2 network trained on Open Images V4*\n",
    "\n",
    "#### 3 models don't have any reference info: \n",
    "\n",
    "ganeval-cifar10-convnet 1 https://aihub.cloud.google.com/p/products%2F110689d8-c594-49d6-aef4-9fc91c9c2ab4\n",
    "\n",
    "random-nnlm-en-dim50 1 https://aihub.cloud.google.com/p/products%2Fb0766fd5-82b5-48e3-9d6f-26c5d2601d5f\n",
    "\n",
    "random-nnlm-en-dim128 1 https://aihub.cloud.google.com/p/products%2Fa52e9466-f691-4f91-a62e-f6e39eb967f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 6 316 2 5 16\n"
     ]
    }
   ],
   "source": [
    "print(len(set_eference), len(set_image_augmentation), len(set_imagenet), len(set_inaturalist_inception_v3), len(set_others), len(set_tf2_preview))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In following cells, we will get some list of models and papers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_reference_withUrl.md\", \"w\") as mrw:\n",
    "    with open(\"model_reference_noUrl.md\", \"w\") as mrn:\n",
    "        \n",
    "        counter_with = 0\n",
    "        counter_no = 0\n",
    "        keys = list(model_paper.keys())\n",
    "        keys.sort()\n",
    "        \n",
    "        for model in keys:\n",
    "            \n",
    "            content = model_paper[model]\n",
    "            \n",
    "            if type(content) == dict:\n",
    "                counter_with += 1\n",
    "                mrw.write(\"## {}. {}\\n\".format(counter_with, model))\n",
    "                mrw.write(\"* {}: {}\\n\".format(\"link_to_model\", model_url[model]))\n",
    "                for ele in content:\n",
    "                    mrw.write(\"* {}: {}\\n\".format(ele, content[ele]))\n",
    "                mrw.write(\"\\n\")\n",
    "                \n",
    "            elif type(content) == str:\n",
    "                counter_no += 1\n",
    "                mrn.write(\"## {}. {}\\n\".format(counter_no, model))                \n",
    "                mrn.write(\"* {}: {}\\n\".format(\"link_to_model\", model_url[model]))\n",
    "                mrn.write(\"* paper: {}\\n\".format(content))\n",
    "                mrn.write(\"* link_to_paper: None. You have to find it manually. \\n\")\n",
    "                mrn.write(\"\\n\")\n",
    "        \n",
    "#         mri.write(\"{}\\n{}\\n\\n\\n\\n\\n\".format(model, model_paper[model])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_model_url = {}\n",
    "paper_model_noUrl = {}\n",
    "\n",
    "for model in model_paper:\n",
    "    content = model_paper[model]\n",
    "    if type(content) == dict:\n",
    "        paper = content[\"paper\"]\n",
    "        if paper in paper_model_url:\n",
    "            paper_model_url[paper].append(model)\n",
    "        else:\n",
    "            paper_model_url[paper] = [model]\n",
    "        pass\n",
    "    elif type(content) == str:\n",
    "        paper = content\n",
    "        if paper in paper_model_noUrl:\n",
    "            paper_model_noUrl[paper].append(model)\n",
    "        else:\n",
    "            paper_model_noUrl[paper] = [model]\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need :\n",
    "* paper name\n",
    "* paper url\n",
    "* model list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "paper_model_url"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "paper_model_noUrl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_url"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "paper_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_paper_model_url_into_file(file):\n",
    "    with open(file, \"w\") as f:\n",
    "        counter = 0\n",
    "        for paper in paper_model_url:\n",
    "            counter += 1\n",
    "            f.write(\"## {}. {}\\n\".format(counter, paper))\n",
    "            f.write(\"* {}: {}\\n\".format(\"link\", paper_url[paper]))\n",
    "            f.write(\"* {}: \\n\".format(\"models\"))\n",
    "            paper_model_url[paper].sort()\n",
    "            for model in paper_model_url[paper]:\n",
    "                f.write(\"    * {} {}\\n\".format(model, model_url[model]))\n",
    "            f.write(\"\\n\")\n",
    "print_paper_model_url_into_file(\"paper_model_url.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_paper_model_noUrl_into_file(file):\n",
    "    with open(file, \"w\") as f:\n",
    "        counter = 0\n",
    "        for paper in paper_model_noUrl:\n",
    "            counter += 1\n",
    "            f.write(\"## {}. {}\\n\".format(counter, paper))\n",
    "            f.write(\"* {}: \\n\".format(\"models\"))\n",
    "            paper_model_noUrl[paper].sort()\n",
    "            for model in paper_model_noUrl[paper]:\n",
    "                f.write(\"    * {} {}\\n\".format(model, model_url[model]))\n",
    "            f.write(\"\\n\")\n",
    "print_paper_model_noUrl_into_file(\"paper_model_noUrl.md\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(\"paper_model_url.md\", \"w\") as mrw:\n",
    "    with open(\"paper_model_noUrl.md\", \"w\") as mrn:\n",
    "        \n",
    "        counter_with = 0\n",
    "        counter_no = 0\n",
    "        \n",
    "        for model in model_paper:\n",
    "            \n",
    "            content = model_paper[model]\n",
    "            \n",
    "            if type(content) == dict:\n",
    "                counter_with += 1\n",
    "                mrw.write(\"## {}. {}\\n\".format(counter_with, model))\n",
    "                for ele in content:\n",
    "                    mrw.write(\"* {}: {}\\n\".format(ele, content[ele]))\n",
    "                mrw.write(\"\\n\")\n",
    "                \n",
    "            elif type(content) == str:\n",
    "                counter_no += 1\n",
    "                mrn.write(\"## {}. {}\\n\".format(counter_no, model))\n",
    "                mrn.write(\"* paper: {}\\n\".format(content))\n",
    "                mrn.write(\"* link_to_paper: None. You have to find it manually. \\n\")\n",
    "                mrn.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
